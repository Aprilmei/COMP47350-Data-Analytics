{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework2\n",
    "\n",
    "Please upload to Moodle a .zip archive containing your Jupyter Notebook with solutions and all data required to reproduce your solutions. \n",
    "\n",
    "Please also prepare a requirements.txt file which lists all the packages that you have used for your homework, one package per line (e.g., pandas). This will allow us to install all required packages in one go, by using \"pip install -r requirements.txt\".\n",
    "\n",
    "Please name your .zip archive using your full name and student id as follows - *Firstnme_Lastname_12345678_COMP47350_Homework2.zip*. \n",
    "\n",
    "For your Notebook, please split the code and explanations into many little cells so it is easy to see and read the results of each step of your solution. Please remember to name your variables and methods with self-explanatory names. Please remember to write comments and where needed, justifications, for the decisions you make and code you write. Feel free to revisit *tips_to_keep_your_ipython_notebook_readable_and_easy_to_debug.html* provided on Moodle.\n",
    "\n",
    "Your code and analysis is like a story that awaits to be read, make it a nice story please!\n",
    "\n",
    "The accepted file formats for the homework are:\n",
    "    - .ipynb\n",
    "    - .zip\n",
    "    - .pdf\n",
    "    - .csv\n",
    "    - .txt\n",
    "    - .html\n",
    "Please keep the whole code in a single notebook. Usage of external tools/files is discouraged for portability reasons. Files in any other format but mentioned above can be used but will be ignored and not considered for the submission (including .doc, .rar, .7z, .pages, .xlsx, .tex etc.). \n",
    "Any image format is allowed to be used as far as the images appear embedded in your report (.ipynb or .pdf or .html).\n",
    "\n",
    "**Deadline: Sunday, April 16, 2017, midnight.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "This homework focuses on building and evaluating prediction models for a particular problem and dataset.\n",
    "The problem and data come from the Amazon online shopping platform. Several sellers can sell the same product on Amazon. Based on the data provided by the seller to Amazon (seller reputation, product price, shipping details, etc) Amazon ranks seller offers from best to worst for a given product. This ranking is mostly influenced by the product price offer of the seller, but it can also be influenced by other features. We first need to understand which features are most indicative of a seller being ranked first by Amazon for a product. When the seller is ranked first for a product we say that the seller is the 'winner' among all the offers, because their offer is shown first when a user searches for a product on Amazon, which increases their chances of selling the product. Our goal is to work with the data to build and evaluate prediction models that capture the relationship between descriptive features and the target feature 'IsWinner'.\n",
    "\n",
    "We use the same dataset from Homework1 (you can use your cleaned/prepared CSV or the raw dataset), a CSV file describing offers by given sellers for given products and a column which records whether an offer was a winner or not.\n",
    "\n",
    "(1). [25] Data Understanding: Exploring relationships between feature pairs:\n",
    "    - (1.1) [5] Print the correlations between the continuous features.\n",
    "    - (1.2) [5] Plot the scatter plots of each pair of continuous descriptive feature and target feature.\n",
    "    - (1.3) [5] Discuss what you observe from the scatter plots and correlations, e.g., which continuous features seem to be better at predicting the target feature. Choose a subset of continuous features you find promising. Justify your choices.\n",
    "    - (1.4) [5] For each categorical feature, plot the pairwise interaction with the target feature (barplots or stacked barplots).\n",
    "    - (1.5) [5] Discuss what knowledge you gain from plotting the interaction of descriptive categorical features and the target feature, e.g., which categorical features seem to be better at predicting the target feature. Choose a subset of categorical features you find promising. Justify your choices.\n",
    "    \n",
    "(2). [15] Predictive Modeling: Linear Regression  \n",
    "    - (2.1) [5] Train a linear regression model to predict the target feature IsWinner, using the descriptive features selected in exercise (1). Evaluate the quality of the model on the training set.   \n",
    "    - (2.2) [2.5] Print the coefficients learned by the model and discuss their statistical significance as well as their role in the model (e.g., interpret the model).    \n",
    "    - (2.3) [2.5] Retrain the model using only the subset of features found to be statistically significant. Evaluate the quality of the model on the training set.\n",
    "    - (2.4) [5] Using the better model (as per evaluation on training set), print the predicted target feature value for all the examples in the training set. Threshold the predicted target feature value at 0.5 to get the predicted class for each example. \n",
    "\n",
    "(3). [15] Predictive Modeling: Logistic Regression  \n",
    "    - (3.1) [5] Train a logistic regression model to predict the target feature IsWinner, using the descriptive features selected in exercise (1). Evaluate the quality of the model on the training set.   \n",
    "    - (3.2) [5] Print the coefficients learned by the model and discuss their statistical significance as well as their role in the model (e.g., interpret the model).    \n",
    "    - (3.3) [2.5] Retrain the model using only the subset of features found to be statistically significant. Evaluate the quality of the model on the training set.\n",
    "    - (3.4) [2.5] Using the better model (as per evaluation on training set), print the predicted target feature value for all the examples in the training set. Print the predicted class for each example.\n",
    "    \n",
    "(4). [20] Predictive Modeling: Random Forest \n",
    "    - (4.1) [5] Train a random forest model to predict the target feature IsWinner, using the descriptive features selected in exercise (1). Evaluate the quality of the model on the training set.   \n",
    "    - (4.2) [5] Print the features ranked by random forest importance. Discuss your findings and choose a subset of features you find promising.\n",
    "    - (4.3) [5] Retrain the model using only the subset of features found to be promising. Evaluate the quality of the model on the training set.\n",
    "    - (4.4) [5] Using the better model (as per evaluation on training set), print the predicted target feature value for all the examples in the training set. Print the predicted class for each example.\n",
    "    \n",
    "(5). [25] Evaluating Predictive Models\n",
    "    - (5.1) [10] Split the dataset into 70% training and remaining 30% test. Train all models from the previous exercises using the new training set and evaluate their quality on the new test set. Print classification evaluation metrics for all models on the test set (e.g., Accuracy, Confusion matrix, Precision, Recall, F1). Discuss how does evaluation on the test set compare to evaluation using the full data for training and also for testing.\n",
    "    - (5.2) [15] Summarize and try to improve your results so far:\n",
    "        - (5.2.1) [5] Which model performs best and is it more accurate than a simple (but useless) model that always predicts IsWinner=0? Justify your answers.\n",
    "        - (5.2.2) [10] Discuss your understanding of the problem and predictive modeling results so far. Can you find any tricks to improve the best model so far (e.g., using feature significance, feature re-scaling, creating new features, combining models, or other knowledge)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1.Data Understanding\n",
    "## Exploring relationships between feature pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>IsWinner</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>TimeOfOfferChange</th>\n",
       "      <th>IsFeaturedMerchant</th>\n",
       "      <th>IsFulfilledByAmazon</th>\n",
       "      <th>ListingPrice</th>\n",
       "      <th>SellerFeedbackRating</th>\n",
       "      <th>SellerFeedbackCount</th>\n",
       "      <th>SellerId</th>\n",
       "      <th>ShippingPrice</th>\n",
       "      <th>ShippingTime_minHours</th>\n",
       "      <th>ShippingTime_maxHours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>94.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1207135739277432339</td>\n",
       "      <td>0.00</td>\n",
       "      <td>672</td>\n",
       "      <td>1008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>107.35</td>\n",
       "      <td>95</td>\n",
       "      <td>4078</td>\n",
       "      <td>-1789487307643024748</td>\n",
       "      <td>0.00</td>\n",
       "      <td>48</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100.46</td>\n",
       "      <td>98</td>\n",
       "      <td>478</td>\n",
       "      <td>5452082314297826053</td>\n",
       "      <td>6.99</td>\n",
       "      <td>24</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>99.24</td>\n",
       "      <td>95</td>\n",
       "      <td>4384</td>\n",
       "      <td>-2572277640783537773</td>\n",
       "      <td>11.67</td>\n",
       "      <td>24</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>109.48</td>\n",
       "      <td>94</td>\n",
       "      <td>105</td>\n",
       "      <td>-8704029307873847986</td>\n",
       "      <td>8.99</td>\n",
       "      <td>24</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  IsWinner            ProductId         TimeOfOfferChange  \\\n",
       "0           0         0 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "1           1         1 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "2           2         0 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "3           3         0 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "4           4         0 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "\n",
       "   IsFeaturedMerchant  IsFulfilledByAmazon  ListingPrice  \\\n",
       "0                   1                    1         94.00   \n",
       "1                   1                    0        107.35   \n",
       "2                   1                    0        100.46   \n",
       "3                   1                    0         99.24   \n",
       "4                   0                    0        109.48   \n",
       "\n",
       "   SellerFeedbackRating  SellerFeedbackCount             SellerId  \\\n",
       "0                     0                    0  1207135739277432339   \n",
       "1                    95                 4078 -1789487307643024748   \n",
       "2                    98                  478  5452082314297826053   \n",
       "3                    95                 4384 -2572277640783537773   \n",
       "4                    94                  105 -8704029307873847986   \n",
       "\n",
       "   ShippingPrice  ShippingTime_minHours  ShippingTime_maxHours  \n",
       "0           0.00                    672                   1008  \n",
       "1           0.00                     48                     72  \n",
       "2           6.99                     24                     48  \n",
       "3          11.67                     24                     48  \n",
       "4           8.99                     24                     48  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the required packages\n",
    "\n",
    "#Import package pandas for data analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Import package numpy for numeric computing\n",
    "import numpy as np\n",
    "\n",
    "# Import package matplotlib  for visualisation/plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "#import package matplotlib for import plots to PDF\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "\n",
    "# Allows plots to appear directly in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "#import package seaborn for visualisation\n",
    "import seaborn as sns\n",
    "\n",
    "# Reading from a csv file, into a data frame\n",
    "df_raw = pd.read_csv('amazon-offers-10k-samples-raw.csv')\n",
    "df_clean = pd.read_csv('amazon_clean_data.csv')\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/April/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#import sklearn for LogisticRegression\n",
    "from patsy import dmatrices\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "#import sklearn for RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Found the first line showed the column we don't need, so drop it \n",
    "df_clean.drop(['Unnamed: 0'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IsWinner</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>TimeOfOfferChange</th>\n",
       "      <th>IsFeaturedMerchant</th>\n",
       "      <th>IsFulfilledByAmazon</th>\n",
       "      <th>ListingPrice</th>\n",
       "      <th>SellerFeedbackRating</th>\n",
       "      <th>SellerFeedbackCount</th>\n",
       "      <th>SellerId</th>\n",
       "      <th>ShippingPrice</th>\n",
       "      <th>ShippingTime_minHours</th>\n",
       "      <th>ShippingTime_maxHours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>94.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1207135739277432339</td>\n",
       "      <td>0.00</td>\n",
       "      <td>672</td>\n",
       "      <td>1008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>107.35</td>\n",
       "      <td>95</td>\n",
       "      <td>4078</td>\n",
       "      <td>-1789487307643024748</td>\n",
       "      <td>0.00</td>\n",
       "      <td>48</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100.46</td>\n",
       "      <td>98</td>\n",
       "      <td>478</td>\n",
       "      <td>5452082314297826053</td>\n",
       "      <td>6.99</td>\n",
       "      <td>24</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>99.24</td>\n",
       "      <td>95</td>\n",
       "      <td>4384</td>\n",
       "      <td>-2572277640783537773</td>\n",
       "      <td>11.67</td>\n",
       "      <td>24</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>109.48</td>\n",
       "      <td>94</td>\n",
       "      <td>105</td>\n",
       "      <td>-8704029307873847986</td>\n",
       "      <td>8.99</td>\n",
       "      <td>24</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IsWinner            ProductId         TimeOfOfferChange  \\\n",
       "0         0 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "1         1 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "2         0 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "3         0 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "4         0 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "\n",
       "   IsFeaturedMerchant  IsFulfilledByAmazon  ListingPrice  \\\n",
       "0                   1                    1         94.00   \n",
       "1                   1                    0        107.35   \n",
       "2                   1                    0        100.46   \n",
       "3                   1                    0         99.24   \n",
       "4                   0                    0        109.48   \n",
       "\n",
       "   SellerFeedbackRating  SellerFeedbackCount             SellerId  \\\n",
       "0                     0                    0  1207135739277432339   \n",
       "1                    95                 4078 -1789487307643024748   \n",
       "2                    98                  478  5452082314297826053   \n",
       "3                    95                 4384 -2572277640783537773   \n",
       "4                    94                  105 -8704029307873847986   \n",
       "\n",
       "   ShippingPrice  ShippingTime_minHours  ShippingTime_maxHours  \n",
       "0           0.00                    672                   1008  \n",
       "1           0.00                     48                     72  \n",
       "2           6.99                     24                     48  \n",
       "3          11.67                     24                     48  \n",
       "4           8.99                     24                     48  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9886, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1Print the correlations between the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ListingPrice</th>\n",
       "      <th>SellerFeedbackRating</th>\n",
       "      <th>SellerFeedbackCount</th>\n",
       "      <th>ShippingPrice</th>\n",
       "      <th>ShippingTime_minHours</th>\n",
       "      <th>ShippingTime_maxHours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ListingPrice</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034981</td>\n",
       "      <td>-0.033080</td>\n",
       "      <td>0.150274</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>-0.000209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SellerFeedbackRating</th>\n",
       "      <td>0.034981</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167111</td>\n",
       "      <td>0.060349</td>\n",
       "      <td>-0.375718</td>\n",
       "      <td>-0.381365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SellerFeedbackCount</th>\n",
       "      <td>-0.033080</td>\n",
       "      <td>0.167111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.067493</td>\n",
       "      <td>-0.001627</td>\n",
       "      <td>-0.030522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ShippingPrice</th>\n",
       "      <td>0.150274</td>\n",
       "      <td>0.060349</td>\n",
       "      <td>-0.067493</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019246</td>\n",
       "      <td>0.019848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ShippingTime_minHours</th>\n",
       "      <td>0.000290</td>\n",
       "      <td>-0.375718</td>\n",
       "      <td>-0.001627</td>\n",
       "      <td>0.019246</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ShippingTime_maxHours</th>\n",
       "      <td>-0.000209</td>\n",
       "      <td>-0.381365</td>\n",
       "      <td>-0.030522</td>\n",
       "      <td>0.019848</td>\n",
       "      <td>0.992427</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ListingPrice  SellerFeedbackRating  \\\n",
       "ListingPrice               1.000000              0.034981   \n",
       "SellerFeedbackRating       0.034981              1.000000   \n",
       "SellerFeedbackCount       -0.033080              0.167111   \n",
       "ShippingPrice              0.150274              0.060349   \n",
       "ShippingTime_minHours      0.000290             -0.375718   \n",
       "ShippingTime_maxHours     -0.000209             -0.381365   \n",
       "\n",
       "                       SellerFeedbackCount  ShippingPrice  \\\n",
       "ListingPrice                     -0.033080       0.150274   \n",
       "SellerFeedbackRating              0.167111       0.060349   \n",
       "SellerFeedbackCount               1.000000      -0.067493   \n",
       "ShippingPrice                    -0.067493       1.000000   \n",
       "ShippingTime_minHours            -0.001627       0.019246   \n",
       "ShippingTime_maxHours            -0.030522       0.019848   \n",
       "\n",
       "                       ShippingTime_minHours  ShippingTime_maxHours  \n",
       "ListingPrice                        0.000290              -0.000209  \n",
       "SellerFeedbackRating               -0.375718              -0.381365  \n",
       "SellerFeedbackCount                -0.001627              -0.030522  \n",
       "ShippingPrice                       0.019246               0.019848  \n",
       "ShippingTime_minHours               1.000000               0.992427  \n",
       "ShippingTime_maxHours               0.992427               1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find all the continuous features \n",
    "\n",
    "continuous_columns=df_clean[['ListingPrice','SellerFeedbackRating','SellerFeedbackCount','ShippingPrice',\n",
    "                       'ShippingTime_minHours','ShippingTime_maxHours']].columns\n",
    "\n",
    "#df[continuous_columns].dtypes\n",
    "# Descriptive stats for continuous features\n",
    "#df[continuous_columns].describe().T\n",
    "\n",
    "# Look at correlations for all the continuous features.\n",
    "df_clean[continuous_columns].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.5,  1.5,  2.5,  3.5,  4.5,  5.5]),\n",
       " <a list of 6 Text yticklabel objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArUAAAJvCAYAAABlBSkIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1113cdcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the  correlations between the continuous features\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "corr = df_clean[continuous_columns].corr()\n",
    "\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "\n",
    "sns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=1, vmin=-1,\n",
    "            square=True, xticklabels=True, yticklabels=True,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "plt.yticks(rotation = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating_score</th>\n",
       "      <th>IsWinner</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>TimeOfOfferChange</th>\n",
       "      <th>IsFeaturedMerchant</th>\n",
       "      <th>IsFulfilledByAmazon</th>\n",
       "      <th>ListingPrice</th>\n",
       "      <th>SellerFeedbackRating</th>\n",
       "      <th>SellerFeedbackCount</th>\n",
       "      <th>SellerId</th>\n",
       "      <th>ShippingPrice</th>\n",
       "      <th>ShippingTime_minHours</th>\n",
       "      <th>ShippingTime_maxHours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>94.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1207135739277432339</td>\n",
       "      <td>0.00</td>\n",
       "      <td>672</td>\n",
       "      <td>1008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>387410</td>\n",
       "      <td>1</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>107.35</td>\n",
       "      <td>95</td>\n",
       "      <td>4078</td>\n",
       "      <td>-1789487307643024748</td>\n",
       "      <td>0.00</td>\n",
       "      <td>48</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46844</td>\n",
       "      <td>0</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100.46</td>\n",
       "      <td>98</td>\n",
       "      <td>478</td>\n",
       "      <td>5452082314297826053</td>\n",
       "      <td>6.99</td>\n",
       "      <td>24</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>416480</td>\n",
       "      <td>0</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>99.24</td>\n",
       "      <td>95</td>\n",
       "      <td>4384</td>\n",
       "      <td>-2572277640783537773</td>\n",
       "      <td>11.67</td>\n",
       "      <td>24</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9870</td>\n",
       "      <td>0</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>109.48</td>\n",
       "      <td>94</td>\n",
       "      <td>105</td>\n",
       "      <td>-8704029307873847986</td>\n",
       "      <td>8.99</td>\n",
       "      <td>24</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating_score  IsWinner            ProductId         TimeOfOfferChange  \\\n",
       "0             0         0 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "1        387410         1 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "2         46844         0 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "3        416480         0 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "4          9870         0 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "\n",
       "   IsFeaturedMerchant  IsFulfilledByAmazon  ListingPrice  \\\n",
       "0                   1                    1         94.00   \n",
       "1                   1                    0        107.35   \n",
       "2                   1                    0        100.46   \n",
       "3                   1                    0         99.24   \n",
       "4                   0                    0        109.48   \n",
       "\n",
       "   SellerFeedbackRating  SellerFeedbackCount             SellerId  \\\n",
       "0                     0                    0  1207135739277432339   \n",
       "1                    95                 4078 -1789487307643024748   \n",
       "2                    98                  478  5452082314297826053   \n",
       "3                    95                 4384 -2572277640783537773   \n",
       "4                    94                  105 -8704029307873847986   \n",
       "\n",
       "   ShippingPrice  ShippingTime_minHours  ShippingTime_maxHours  \n",
       "0           0.00                    672                   1008  \n",
       "1           0.00                     48                     72  \n",
       "2           6.99                     24                     48  \n",
       "3          11.67                     24                     48  \n",
       "4           8.99                     24                     48  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df_clean\n",
    "\n",
    "df.insert(0,\"Rating_score\", df.SellerFeedbackRating * df.SellerFeedbackCount)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating_score</th>\n",
       "      <th>IsWinner</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>TimeOfOfferChange</th>\n",
       "      <th>IsFeaturedMerchant</th>\n",
       "      <th>IsFulfilledByAmazon</th>\n",
       "      <th>ListingPrice</th>\n",
       "      <th>SellerFeedbackRating</th>\n",
       "      <th>SellerFeedbackCount</th>\n",
       "      <th>SellerId</th>\n",
       "      <th>ShippingPrice</th>\n",
       "      <th>ShippingTime_minHours</th>\n",
       "      <th>ShippingTime_maxHours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1207135739277432339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>387410</td>\n",
       "      <td>1</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.235077</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.098455</td>\n",
       "      <td>-1789487307643024748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.097429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46844</td>\n",
       "      <td>0</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.113752</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.011540</td>\n",
       "      <td>5452082314297826053</td>\n",
       "      <td>0.466311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>416480</td>\n",
       "      <td>0</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.092270</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.105843</td>\n",
       "      <td>-2572277640783537773</td>\n",
       "      <td>0.778519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9870</td>\n",
       "      <td>0</td>\n",
       "      <td>-4885255916155087854</td>\n",
       "      <td>2016-02-04T04:26:52.136Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.272583</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>-8704029307873847986</td>\n",
       "      <td>0.599733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating_score  IsWinner            ProductId         TimeOfOfferChange  \\\n",
       "0             0         0 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "1        387410         1 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "2         46844         0 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "3        416480         0 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "4          9870         0 -4885255916155087854  2016-02-04T04:26:52.136Z   \n",
       "\n",
       "   IsFeaturedMerchant  IsFulfilledByAmazon  ListingPrice  \\\n",
       "0                   1                    1      0.000000   \n",
       "1                   1                    0      0.235077   \n",
       "2                   1                    0      0.113752   \n",
       "3                   1                    0      0.092270   \n",
       "4                   0                    0      0.272583   \n",
       "\n",
       "   SellerFeedbackRating  SellerFeedbackCount             SellerId  \\\n",
       "0                  0.00             0.000000  1207135739277432339   \n",
       "1                  0.95             0.098455 -1789487307643024748   \n",
       "2                  0.98             0.011540  5452082314297826053   \n",
       "3                  0.95             0.105843 -2572277640783537773   \n",
       "4                  0.94             0.002535 -8704029307873847986   \n",
       "\n",
       "   ShippingPrice  ShippingTime_minHours  ShippingTime_maxHours  \n",
       "0       0.000000               1.000000               0.000000  \n",
       "1       0.000000               0.037037               0.097429  \n",
       "2       0.466311               0.000000               0.011781  \n",
       "3       0.778519               0.000000               0.104740  \n",
       "4       0.599733               0.000000               0.002482  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Range normalise all columns\n",
    "#For different products, we groupby them first \n",
    "\n",
    "zscore = lambda x: (x - x.min()) /( x.max()-x.min())\n",
    "#df['ListingPrice']=df_clean.groupby('ProductId','TimeOfOfferChange')['ListingPrice'].transform(zscore)\n",
    "df['ListingPrice']=df.groupby('ProductId')['ListingPrice'].transform(zscore)\n",
    "df['SellerFeedbackRating']=df.groupby('ProductId')['SellerFeedbackRating'].transform(zscore)\n",
    "df['SellerFeedbackCount']=df.groupby('ProductId')['SellerFeedbackCount'].transform(zscore)\n",
    "df['ShippingPrice']=df.groupby('ProductId')['ShippingPrice'].transform(zscore)\n",
    "df['ShippingTime_minHours']=df.groupby('ProductId')['ShippingTime_minHours'].transform(zscore)\n",
    "df['ShippingTime_maxHours']=df.groupby('ProductId')['ShippingTime_maxHours'].transform(zscore)\n",
    "df['ShippingTime_maxHours']=df.groupby('ProductId')['Rating_score'].transform(zscore)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ListingPrice</th>\n",
       "      <th>SellerFeedbackRating</th>\n",
       "      <th>SellerFeedbackCount</th>\n",
       "      <th>ShippingPrice</th>\n",
       "      <th>ShippingTime_minHours</th>\n",
       "      <th>ShippingTime_maxHours</th>\n",
       "      <th>IsWinner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ListingPrice</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062829</td>\n",
       "      <td>0.025886</td>\n",
       "      <td>-0.060375</td>\n",
       "      <td>-0.010152</td>\n",
       "      <td>0.023802</td>\n",
       "      <td>-0.243341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SellerFeedbackRating</th>\n",
       "      <td>0.062829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.145440</td>\n",
       "      <td>0.093869</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>0.147288</td>\n",
       "      <td>-0.276790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SellerFeedbackCount</th>\n",
       "      <td>0.025886</td>\n",
       "      <td>0.145440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144916</td>\n",
       "      <td>0.126780</td>\n",
       "      <td>0.999805</td>\n",
       "      <td>-0.039756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ShippingPrice</th>\n",
       "      <td>-0.060375</td>\n",
       "      <td>0.093869</td>\n",
       "      <td>-0.144916</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077376</td>\n",
       "      <td>-0.146146</td>\n",
       "      <td>-0.147019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ShippingTime_minHours</th>\n",
       "      <td>-0.010152</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>0.126780</td>\n",
       "      <td>0.077376</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.128211</td>\n",
       "      <td>-0.075646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ShippingTime_maxHours</th>\n",
       "      <td>0.023802</td>\n",
       "      <td>0.147288</td>\n",
       "      <td>0.999805</td>\n",
       "      <td>-0.146146</td>\n",
       "      <td>0.128211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.038282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsWinner</th>\n",
       "      <td>-0.243341</td>\n",
       "      <td>-0.276790</td>\n",
       "      <td>-0.039756</td>\n",
       "      <td>-0.147019</td>\n",
       "      <td>-0.075646</td>\n",
       "      <td>-0.038282</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ListingPrice  SellerFeedbackRating  \\\n",
       "ListingPrice               1.000000              0.062829   \n",
       "SellerFeedbackRating       0.062829              1.000000   \n",
       "SellerFeedbackCount        0.025886              0.145440   \n",
       "ShippingPrice             -0.060375              0.093869   \n",
       "ShippingTime_minHours     -0.010152              0.003115   \n",
       "ShippingTime_maxHours      0.023802              0.147288   \n",
       "IsWinner                  -0.243341             -0.276790   \n",
       "\n",
       "                       SellerFeedbackCount  ShippingPrice  \\\n",
       "ListingPrice                      0.025886      -0.060375   \n",
       "SellerFeedbackRating              0.145440       0.093869   \n",
       "SellerFeedbackCount               1.000000      -0.144916   \n",
       "ShippingPrice                    -0.144916       1.000000   \n",
       "ShippingTime_minHours             0.126780       0.077376   \n",
       "ShippingTime_maxHours             0.999805      -0.146146   \n",
       "IsWinner                         -0.039756      -0.147019   \n",
       "\n",
       "                       ShippingTime_minHours  ShippingTime_maxHours  IsWinner  \n",
       "ListingPrice                       -0.010152               0.023802 -0.243341  \n",
       "SellerFeedbackRating                0.003115               0.147288 -0.276790  \n",
       "SellerFeedbackCount                 0.126780               0.999805 -0.039756  \n",
       "ShippingPrice                       0.077376              -0.146146 -0.147019  \n",
       "ShippingTime_minHours               1.000000               0.128211 -0.075646  \n",
       "ShippingTime_maxHours               0.128211               1.000000 -0.038282  \n",
       "IsWinner                           -0.075646              -0.038282  1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add the target feature, we print the \n",
    "df_continue=df[['ListingPrice','SellerFeedbackRating','SellerFeedbackCount','ShippingPrice',\n",
    "                       'ShippingTime_minHours','ShippingTime_maxHours','IsWinner']]\n",
    "df_continue.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Plot the scatter plots of each pair of continuous descriptive feature and target feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3cAAAHfCAYAAADz4yG8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndiChYRsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105766390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 6, sharey=True)\n",
    "df.plot(kind='scatter', x='ShippingTime_maxHours', y='IsWinner', label=\"%.3f\" % df[['ShippingTime_maxHours', 'IsWinner']].corr().as_matrix()[0,1], ax=axs[0], figsize=(15, 8))\n",
    "df.plot(kind='scatter', x='SellerFeedbackRating', y='IsWinner', label=\"%.3f\" % df[['SellerFeedbackRating', 'IsWinner']].corr().as_matrix()[0,1], ax=axs[1])\n",
    "df.plot(kind='scatter', x='SellerFeedbackCount', y='IsWinner', label=\"%.3f\" % df[['SellerFeedbackCount', 'IsWinner']].corr().as_matrix()[0,1], ax=axs[2])\n",
    "df.plot(kind='scatter', x='ListingPrice', y='IsWinner', label=\"%.3f\" % df[['ListingPrice', 'IsWinner']].corr().as_matrix()[0,1], ax=axs[3])\n",
    "df.plot(kind='scatter', x='ShippingPrice', y='IsWinner', label=\"%.3f\" % df[['ShippingPrice', 'IsWinner']].corr().as_matrix()[0,1], ax=axs[4])\n",
    "df.plot(kind='scatter', x='ShippingTime_minHours', y='IsWinner', label=\"%.3f\" % df[['ShippingTime_minHours', 'IsWinner']].corr().as_matrix()[0,1], ax=axs[5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Analysis Data\n",
    "question:  Discuss what you observe from the scatter plots and correlations, e.g., which continuous features seem to be better at predicting the target feature. Choose a subset of continuous features you find promising. Justify your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: \n",
    "As we could see from the scatter plots, SellerFeedbackRating and ListingPrice have the highest correlations to target feature. These two features are better at predicting the target feature. \n",
    "\n",
    "In the correlations between the continuous features we could see that ShippingTime_minHours  and ShippingTime_maxHour\n",
    "have very high correlation:0.992427, after normolation, it is almost the same. The two features have the same affection to the target feature, so it needs to choose one of them. Here I will choose the ShippingTime_maxHour, first, it has higher correlation to the target feature. Second from business side, I think it is importtant to know the maximum delivery time.  \n",
    "\n",
    "Then we could see that SellerFeedbackCount's correlation to target feature is only -0.039756, it is too close to 0, so we will not use it. \n",
    "At end the continuous features I chose are SellerFeedbackRating + ListingPrice +ShippingPrice+ShippingTime_maxHours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Plot categorical features with target feature \n",
    "For each categorical feature, plot the pairwise interaction with the target feature (barplots or stacked barplots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find all the categorical features \n",
    "categorical_columns = df[['IsWinner','IsFeaturedMerchant',\n",
    "                         'IsFulfilledByAmazon']].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwkAAAGzCAYAAABkVVu2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYEBJAAAAAGBASQAAAABg8P8BIhkSITPHCUMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115e60400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count the number of winner \n",
    "winner_count = 1 / df[df.IsWinner == 1].count()['IsWinner']\n",
    "\n",
    "\n",
    "# Counts the number of male candidates\n",
    "not_Winner_count = 1 / df[df.IsWinner == 0].count()['IsWinner']\n",
    "\n",
    "# Create a new column in the dataframe called percent and insert male_count in all cells\n",
    "df['percent'] = winner_count * 100\n",
    "\n",
    "# Find indexes of all rows containing value Female for Gender\n",
    "index_list = df[df['IsWinner'] == 0].index.tolist()\n",
    "\n",
    "# For each row with a 'female' value, insert female_count in the percent column\n",
    "for i in index_list:\n",
    "    df.loc[i, 'percent'] = not_Winner_count * 100\n",
    "\n",
    "# Group dataframe by Gender and Elected and sum precent\n",
    "category_group = df[['percent','IsWinner','IsFeaturedMerchant']].groupby(['IsWinner','IsFeaturedMerchant']).sum()\n",
    "\n",
    "# Plot values of category_group in a stacked bar chart\n",
    "my_plot = category_group.unstack().plot(kind='bar', stacked=True, title=\"IsFeaturedMerchant by IsWinner\", figsize=(13,7))\n",
    "\n",
    "# Define legend colours and text and add to the plot\n",
    "red_patch = mpatches.Patch(color='green', label='IsFeaturedMerchant')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Not FeaturedMerchant')\n",
    "my_plot.legend(handles=[red_patch, blue_patch], frameon = True)\n",
    "\n",
    "# Define x and y labels and min and max values for the y axis\n",
    "my_plot.set_xlabel(\"IsWinner\")\n",
    "my_plot.set_ylabel(\"% IsFeaturedMerchant\")\n",
    "my_plot.set_ylim([0,100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwkAAAGzCAYAAABkVVu2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11550fa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Group dataframe by Gender and Elected and sum precent\n",
    "category_group = df[['percent','IsWinner','IsFulfilledByAmazon']].groupby(['IsWinner','IsFulfilledByAmazon']).sum()\n",
    "\n",
    "# Plot values of category_group in a stacked bar chart\n",
    "my_plot = category_group.unstack().plot(kind='bar', stacked=True, title=\"IsFulfilledByAmazon by IsWinner\", figsize=(13,7))\n",
    "\n",
    "# Define legend colours and text and add to the plot\n",
    "red_patch = mpatches.Patch(color='green', label='IsFulfilledByAmazon')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Not FulfilledByAmazon')\n",
    "my_plot.legend(handles=[red_patch, blue_patch], frameon = True)\n",
    "\n",
    "# Define x and y labels and min and max values for the y axis\n",
    "my_plot.set_xlabel(\"IsWinner\")\n",
    "my_plot.set_ylabel(\"% IsFulfilledByAmazon\")\n",
    "my_plot.set_ylim([0,100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Analysis Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Question:  Discuss what knowledge you gain from plotting the interaction of descriptive categorical features and the target feature, e.g., which categorical features seem to be better at predicting the target feature. Choose a subset of categorical features you find promising. Justify your choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "From the plots we could see that IsFulfilledByAmazon is better to predict the target feature. Because When IsWinner feature equal to 1, IsFeaturedMerchant feature  is always the same value 1. As both features have related to target feature, I kept both of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Predictive Modeling: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.1 Train a linear regression model to predict the target feature IsWinner, using the descriptive features selected in exercise (1). Evaluate the quality of the model on the training set.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept                0.090401\n",
      "SellerFeedbackRating    -0.031337\n",
      "ListingPrice            -0.135204\n",
      "ShippingPrice           -0.061845\n",
      "ShippingTime_maxHours   -0.012599\n",
      "IsFulfilledByAmazon      0.469942\n",
      "IsFeaturedMerchant       0.056710\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Import statsmodels package for training a linear regression model.\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "# Train the model\n",
    "lm = sm.ols(formula=\"IsWinner ~  SellerFeedbackRating + ListingPrice +ShippingPrice+ShippingTime_maxHours+IsFulfilledByAmazon+IsFeaturedMerchant\", data=df).fit()\n",
    "\n",
    "# Print the weights learned for each feature.\n",
    "print(lm.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               IsWinner   R-squared:                       0.260\n",
      "Model:                            OLS   Adj. R-squared:                  0.259\n",
      "Method:                 Least Squares   F-statistic:                     577.3\n",
      "Date:                Tue, 25 Apr 2017   Prob (F-statistic):               0.00\n",
      "Time:                        15:43:44   Log-Likelihood:                 2046.5\n",
      "No. Observations:                9886   AIC:                            -4079.\n",
      "Df Residuals:                    9879   BIC:                            -4029.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "Intercept                 0.0904      0.009      9.637      0.000       0.072       0.109\n",
      "SellerFeedbackRating     -0.0313      0.009     -3.367      0.001      -0.050      -0.013\n",
      "ListingPrice             -0.1352      0.007    -20.061      0.000      -0.148      -0.122\n",
      "ShippingPrice            -0.0618      0.007     -8.916      0.000      -0.075      -0.048\n",
      "ShippingTime_maxHours    -0.0126      0.008     -1.640      0.101      -0.028       0.002\n",
      "IsFulfilledByAmazon       0.4699      0.014     34.376      0.000       0.443       0.497\n",
      "IsFeaturedMerchant        0.0567      0.005     10.452      0.000       0.046       0.067\n",
      "==============================================================================\n",
      "Omnibus:                     6424.894   Durbin-Watson:                   2.238\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            89317.932\n",
      "Skew:                           2.959   Prob(JB):                         0.00\n",
      "Kurtosis:                      16.484   Cond. No.                         13.9\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Only feature Size is found to be statistically significant (p-value is smaller than 0.05, p-value=0).\n",
    "# If the 95% confidence interval includes zero, the p-value for that coefficient will be greater than 0.05.\n",
    "# Print the summary of the trained model and analysis the result\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_input is the input for the model \n",
    "df_input=df[['ListingPrice','SellerFeedbackRating','ShippingPrice','ShippingTime_maxHours','IsFulfilledByAmazon','IsFeaturedMerchant']]\n",
    "\n",
    "# predicted is the result of the model predicted for all the data in\n",
    "predicted=lm.predict(df_input)\n",
    "\n",
    "predictions=[]\n",
    "\n",
    "for i in predicted:\n",
    "    if i>0.5:\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "        \n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.952255715153\n",
      "Confusion matrix: \n",
      " [[9193  146]\n",
      " [ 326  221]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.98      0.97      9339\n",
      "          1       0.60      0.40      0.48       547\n",
      "\n",
      "avg / total       0.95      0.95      0.95      9886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some more evaluation metrics.\n",
    "predictions = pd.DataFrame({'predicted': predictions})\n",
    "y=df[['IsWinner']]\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y, predictions))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y, predictions))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the quality of the model on the training set.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Classification report we could see that the model didn't predict IsWinner well. The precision of IsWineer==1 is only 0.6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Print the coefficients learned by the model and discuss their statistical significance as well as their role in the model (e.g., interpret the model).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept                0.090401\n",
      "SellerFeedbackRating    -0.031337\n",
      "ListingPrice            -0.135204\n",
      "ShippingPrice           -0.061845\n",
      "ShippingTime_maxHours   -0.012599\n",
      "IsFulfilledByAmazon      0.469942\n",
      "IsFeaturedMerchant       0.056710\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print the model weights/parameters\n",
    "print(lm.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model, IsFulfilledByAmazon's weight is 0.491253 and it could change the target feature more than the other features. SellerFeedbackRating has a very low weight: -0.023587 means that it doesn't so relate to the target feature. \n",
    "We could see that in the summary of the trained model, the P value of the SellerFeedbackRating  0.011+0.05 is the only bigger  than 0.05. P-value shows\tsignificance of feature, the p-value of SellerFeedbackRating shows that it doesn't hava\tasig nificant impact on\tthe\tmodel. The other features have significance impact on the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Retrain the model using only the subset of features found to be statistically significant. Evaluate the quality of the model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept                0.065399\n",
      "ListingPrice            -0.133399\n",
      "ShippingPrice           -0.061646\n",
      "ShippingTime_maxHours   -0.013895\n",
      "IsFulfilledByAmazon      0.496658\n",
      "IsFeaturedMerchant       0.053882\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# retrain the model with ListingPrice+ShippingPrice+ShippingTime_maxHours+IsFulfilledByAmazon+IsFeaturedMerchant\n",
    "lm = sm.ols(formula=\"IsWinner ~  ListingPrice+ShippingPrice+ShippingTime_maxHours+IsFulfilledByAmazon+IsFeaturedMerchant\", data=df).fit()\n",
    "\n",
    "# Print the model weights/parameters\n",
    "print(lm.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               IsWinner   R-squared:                       0.259\n",
      "Model:                            OLS   Adj. R-squared:                  0.258\n",
      "Method:                 Least Squares   F-statistic:                     689.7\n",
      "Date:                Tue, 25 Apr 2017   Prob (F-statistic):               0.00\n",
      "Time:                        15:43:44   Log-Likelihood:                 2040.8\n",
      "No. Observations:                9886   AIC:                            -4070.\n",
      "Df Residuals:                    9880   BIC:                            -4026.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "Intercept                 0.0654      0.006     11.402      0.000       0.054       0.077\n",
      "ListingPrice             -0.1334      0.007    -19.845      0.000      -0.147      -0.120\n",
      "ShippingPrice            -0.0616      0.007     -8.883      0.000      -0.075      -0.048\n",
      "ShippingTime_maxHours    -0.0139      0.008     -1.809      0.070      -0.029       0.001\n",
      "IsFulfilledByAmazon       0.4967      0.011     44.590      0.000       0.475       0.518\n",
      "IsFeaturedMerchant        0.0539      0.005     10.047      0.000       0.043       0.064\n",
      "==============================================================================\n",
      "Omnibus:                     6441.982   Durbin-Watson:                   2.235\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            89918.441\n",
      "Skew:                           2.969   Prob(JB):                         0.00\n",
      "Kurtosis:                      16.529   Cond. No.                         8.44\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Using the better model (as per evaluation on training set), print the predicted target feature value for all the examples in the training set. Threshold the predicted target feature value at 0.5 to get the predicted class for each example. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_input=df[['ListingPrice','ShippingPrice','ShippingTime_maxHours','IsFulfilledByAmazon','IsFeaturedMerchant']]\n",
    "#predicted=lm.predict(df_input)\n",
    "#predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=[]\n",
    "\n",
    "for i in lm.predict(df_input):\n",
    "    if i>0.5:\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "        \n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ListingPrice</th>\n",
       "      <th>ShippingPrice</th>\n",
       "      <th>ShippingTime_maxHours</th>\n",
       "      <th>IsFulfilledByAmazon</th>\n",
       "      <th>IsFeaturedMerchant</th>\n",
       "      <th>IsWinner</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.235077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097429</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.113752</td>\n",
       "      <td>0.466311</td>\n",
       "      <td>0.011781</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.092270</td>\n",
       "      <td>0.778519</td>\n",
       "      <td>0.104740</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.272583</td>\n",
       "      <td>0.599733</td>\n",
       "      <td>0.002482</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.399718</td>\n",
       "      <td>0.465644</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.540412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115710</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.459236</td>\n",
       "      <td>0.665777</td>\n",
       "      <td>0.043216</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.558725</td>\n",
       "      <td>0.333556</td>\n",
       "      <td>0.242723</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.480190</td>\n",
       "      <td>0.665777</td>\n",
       "      <td>0.043216</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.688854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851376</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.524212</td>\n",
       "      <td>0.863909</td>\n",
       "      <td>0.171173</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.802606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004208</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.752069</td>\n",
       "      <td>0.369580</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.655925</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007967</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.814580</td>\n",
       "      <td>0.466978</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.914422</td>\n",
       "      <td>0.089393</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.990843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.015816</td>\n",
       "      <td>0.217796</td>\n",
       "      <td>0.081159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.053887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851376</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.058109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.025680</td>\n",
       "      <td>0.197780</td>\n",
       "      <td>0.043216</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.029383</td>\n",
       "      <td>0.197780</td>\n",
       "      <td>0.043216</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.429845</td>\n",
       "      <td>0.058962</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.040424</td>\n",
       "      <td>0.224138</td>\n",
       "      <td>0.104740</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.069461</td>\n",
       "      <td>0.099088</td>\n",
       "      <td>0.242723</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.080916</td>\n",
       "      <td>0.099088</td>\n",
       "      <td>0.242723</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.099121</td>\n",
       "      <td>0.261395</td>\n",
       "      <td>0.187051</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.121340</td>\n",
       "      <td>0.138724</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9856</th>\n",
       "      <td>0.061013</td>\n",
       "      <td>0.296529</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9857</th>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.222844</td>\n",
       "      <td>0.171173</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9858</th>\n",
       "      <td>0.254797</td>\n",
       "      <td>0.111982</td>\n",
       "      <td>0.242723</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9859</th>\n",
       "      <td>0.074188</td>\n",
       "      <td>0.475924</td>\n",
       "      <td>0.058962</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9860</th>\n",
       "      <td>0.408416</td>\n",
       "      <td>0.154311</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9861</th>\n",
       "      <td>0.468790</td>\n",
       "      <td>0.167973</td>\n",
       "      <td>0.273024</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9862</th>\n",
       "      <td>0.520594</td>\n",
       "      <td>0.265174</td>\n",
       "      <td>0.187051</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9863</th>\n",
       "      <td>0.698516</td>\n",
       "      <td>0.134378</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9864</th>\n",
       "      <td>0.635201</td>\n",
       "      <td>0.341769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9865</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035638</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9866</th>\n",
       "      <td>0.697109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004442</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9867</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251288</td>\n",
       "      <td>0.104764</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9868</th>\n",
       "      <td>0.050013</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>0.081159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9869</th>\n",
       "      <td>0.037350</td>\n",
       "      <td>0.201344</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9870</th>\n",
       "      <td>0.160527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9871</th>\n",
       "      <td>0.161678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851569</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9872</th>\n",
       "      <td>0.039908</td>\n",
       "      <td>0.223516</td>\n",
       "      <td>0.043216</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9873</th>\n",
       "      <td>0.178946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004208</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9874</th>\n",
       "      <td>0.182144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115710</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9875</th>\n",
       "      <td>0.058199</td>\n",
       "      <td>0.223516</td>\n",
       "      <td>0.043216</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9876</th>\n",
       "      <td>0.061013</td>\n",
       "      <td>0.296529</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9877</th>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.222844</td>\n",
       "      <td>0.171173</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9878</th>\n",
       "      <td>0.254797</td>\n",
       "      <td>0.111982</td>\n",
       "      <td>0.242723</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9879</th>\n",
       "      <td>0.074188</td>\n",
       "      <td>0.475924</td>\n",
       "      <td>0.058962</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9880</th>\n",
       "      <td>0.408416</td>\n",
       "      <td>0.154311</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9881</th>\n",
       "      <td>0.468790</td>\n",
       "      <td>0.167973</td>\n",
       "      <td>0.273024</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9882</th>\n",
       "      <td>0.520594</td>\n",
       "      <td>0.265174</td>\n",
       "      <td>0.187051</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9883</th>\n",
       "      <td>0.698516</td>\n",
       "      <td>0.134378</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9884</th>\n",
       "      <td>0.635201</td>\n",
       "      <td>0.341769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9885</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035638</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9886 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ListingPrice  ShippingPrice  ShippingTime_maxHours  IsFulfilledByAmazon  \\\n",
       "0         0.000000       0.000000               0.000000                    1   \n",
       "1         0.235077       0.000000               0.097429                    0   \n",
       "2         0.113752       0.466311               0.011781                    0   \n",
       "3         0.092270       0.778519               0.104740                    0   \n",
       "4         0.272583       0.599733               0.002482                    0   \n",
       "5         0.399718       0.465644               0.000152                    0   \n",
       "6         0.540412       0.000000               0.115710                    0   \n",
       "7         0.459236       0.665777               0.043216                    0   \n",
       "8         0.558725       0.333556               0.242723                    0   \n",
       "9         0.480190       0.665777               0.043216                    0   \n",
       "10        0.688854       0.000000               0.851376                    0   \n",
       "11        0.524212       0.863909               0.171173                    0   \n",
       "12        0.802606       0.000000               0.004208                    0   \n",
       "13        0.752069       0.369580               0.000075                    0   \n",
       "14        0.655925       1.000000               0.007967                    0   \n",
       "15        0.814580       0.466978               0.999976                    0   \n",
       "16        0.914422       0.089393               0.000880                    0   \n",
       "17        0.990843       0.000000               0.000503                    0   \n",
       "18        1.000000       0.000000               0.000654                    0   \n",
       "19        0.015816       0.217796               0.081159                    0   \n",
       "20        0.053887       0.000000               0.851376                    0   \n",
       "21        0.058109       0.000000               0.000000                    1   \n",
       "22        0.025680       0.197780               0.043216                    0   \n",
       "23        0.029383       0.197780               0.043216                    0   \n",
       "24        0.000000       0.429845               0.058962                    0   \n",
       "25        0.040424       0.224138               0.104740                    0   \n",
       "26        0.069461       0.099088               0.242723                    0   \n",
       "27        0.080916       0.099088               0.242723                    0   \n",
       "28        0.099121       0.261395               0.187051                    0   \n",
       "29        0.121340       0.138724               0.999976                    0   \n",
       "...            ...            ...                    ...                  ...   \n",
       "9856      0.061013       0.296529               0.009657                    0   \n",
       "9857      0.149015       0.222844               0.171173                    0   \n",
       "9858      0.254797       0.111982               0.242723                    0   \n",
       "9859      0.074188       0.475924               0.058962                    0   \n",
       "9860      0.408416       0.154311               0.000050                    0   \n",
       "9861      0.468790       0.167973               0.273024                    0   \n",
       "9862      0.520594       0.265174               0.187051                    0   \n",
       "9863      0.698516       0.134378               1.000000                    0   \n",
       "9864      0.635201       0.341769               0.000000                    0   \n",
       "9865      1.000000       0.000000               0.035638                    0   \n",
       "9866      0.697109       1.000000               0.004442                    0   \n",
       "9867      0.000000       0.251288               0.104764                    0   \n",
       "9868      0.050013       0.178947               0.081159                    0   \n",
       "9869      0.037350       0.201344               0.003401                    0   \n",
       "9870      0.160527       0.000000               0.000000                    1   \n",
       "9871      0.161678       0.000000               0.851569                    0   \n",
       "9872      0.039908       0.223516               0.043216                    0   \n",
       "9873      0.178946       0.000000               0.004208                    0   \n",
       "9874      0.182144       0.000000               0.115710                    0   \n",
       "9875      0.058199       0.223516               0.043216                    0   \n",
       "9876      0.061013       0.296529               0.009657                    0   \n",
       "9877      0.149015       0.222844               0.171173                    0   \n",
       "9878      0.254797       0.111982               0.242723                    0   \n",
       "9879      0.074188       0.475924               0.058962                    0   \n",
       "9880      0.408416       0.154311               0.000050                    0   \n",
       "9881      0.468790       0.167973               0.273024                    0   \n",
       "9882      0.520594       0.265174               0.187051                    0   \n",
       "9883      0.698516       0.134378               1.000000                    0   \n",
       "9884      0.635201       0.341769               0.000000                    0   \n",
       "9885      1.000000       0.000000               0.035638                    0   \n",
       "\n",
       "      IsFeaturedMerchant  IsWinner  predicted  \n",
       "0                      1         0          1  \n",
       "1                      1         1          0  \n",
       "2                      1         0          0  \n",
       "3                      1         0          0  \n",
       "4                      0         0          0  \n",
       "5                      0         0          0  \n",
       "6                      1         0          0  \n",
       "7                      1         0          0  \n",
       "8                      1         0          0  \n",
       "9                      1         0          0  \n",
       "10                     1         0          0  \n",
       "11                     1         0          0  \n",
       "12                     1         0          0  \n",
       "13                     1         0          0  \n",
       "14                     1         0          0  \n",
       "15                     1         0          0  \n",
       "16                     1         0          0  \n",
       "17                     1         0          0  \n",
       "18                     1         0          0  \n",
       "19                     0         0          0  \n",
       "20                     1         1          0  \n",
       "21                     1         0          1  \n",
       "22                     1         0          0  \n",
       "23                     1         0          0  \n",
       "24                     1         0          0  \n",
       "25                     1         0          0  \n",
       "26                     1         0          0  \n",
       "27                     1         0          0  \n",
       "28                     1         0          0  \n",
       "29                     1         0          0  \n",
       "...                  ...       ...        ...  \n",
       "9856                   1         0          0  \n",
       "9857                   1         0          0  \n",
       "9858                   1         0          0  \n",
       "9859                   1         0          0  \n",
       "9860                   1         0          0  \n",
       "9861                   1         0          0  \n",
       "9862                   1         0          0  \n",
       "9863                   1         0          0  \n",
       "9864                   1         0          0  \n",
       "9865                   1         0          0  \n",
       "9866                   1         0          0  \n",
       "9867                   1         0          0  \n",
       "9868                   0         0          0  \n",
       "9869                   0         0          0  \n",
       "9870                   1         1          1  \n",
       "9871                   1         0          0  \n",
       "9872                   1         0          0  \n",
       "9873                   1         0          0  \n",
       "9874                   1         0          0  \n",
       "9875                   1         0          0  \n",
       "9876                   1         0          0  \n",
       "9877                   1         0          0  \n",
       "9878                   1         0          0  \n",
       "9879                   1         0          0  \n",
       "9880                   1         0          0  \n",
       "9881                   1         0          0  \n",
       "9882                   1         0          0  \n",
       "9883                   1         0          0  \n",
       "9884                   1         0          0  \n",
       "9885                   1         0          0  \n",
       "\n",
       "[9886 rows x 7 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_linear = pd.DataFrame({'predicted': predictions})\n",
    "result_linear= pd.concat([df_input,df[['IsWinner']],predicted_linear], axis=1)\n",
    "result_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.952255715153\n",
      "Confusion matrix: \n",
      " [[9193  146]\n",
      " [ 326  221]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.98      0.97      9339\n",
      "          1       0.60      0.40      0.48       547\n",
      "\n",
      "avg / total       0.95      0.95      0.95      9886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some more evaluation metrics.\n",
    "#predicted_linear = pd.DataFrame({'predicted': predictions})\n",
    "predictions=predicted_linear\n",
    "y=df[['IsWinner']]\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y, predictions))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y, predictions))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Predictive Modeling: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train a logistic regression model to predict the target feature IsWinner, using the descriptive features selected in exercise (1). Evaluate the quality of the model on the training set.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.123630\n",
      "         Iterations: 35\n",
      "Intercept                0.065399\n",
      "ListingPrice            -0.133399\n",
      "ShippingPrice           -0.061646\n",
      "ShippingTime_maxHours   -0.013895\n",
      "IsFulfilledByAmazon      0.496658\n",
      "IsFeaturedMerchant       0.053882\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/April/anaconda/lib/python3.5/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# logistic regression model\n",
    "logreg = sm.logit(formula=\"IsWinner ~  SellerFeedbackRating + ListingPrice+ShippingPrice+ShippingTime_maxHours+IsFulfilledByAmazon+IsFeaturedMerchant\", data=df).fit()\n",
    "\n",
    "# Print the model weights/parameters\n",
    "print(lm.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:               IsWinner   No. Observations:                 9886\n",
      "Model:                          Logit   Df Residuals:                     9879\n",
      "Method:                           MLE   Df Model:                            6\n",
      "Date:                Tue, 25 Apr 2017   Pseudo R-squ.:                  0.4221\n",
      "Time:                        15:43:51   Log-Likelihood:                -1222.2\n",
      "converged:                      False   LL-Null:                       -2114.8\n",
      "                                        LLR p-value:                     0.000\n",
      "=========================================================================================\n",
      "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "Intercept               -20.9564   3747.084     -0.006      0.996   -7365.106    7323.193\n",
      "SellerFeedbackRating     -1.0297      0.292     -3.526      0.000      -1.602      -0.457\n",
      "ListingPrice             -8.5585      0.487    -17.589      0.000      -9.512      -7.605\n",
      "ShippingPrice            -3.4030      0.338    -10.058      0.000      -4.066      -2.740\n",
      "ShippingTime_maxHours    -0.3066      0.219     -1.402      0.161      -0.735       0.122\n",
      "IsFulfilledByAmazon       0.8698      0.289      3.005      0.003       0.303       1.437\n",
      "IsFeaturedMerchant       21.4211   3747.084      0.006      0.995   -7322.728    7365.571\n",
      "=========================================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.23 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "# Print the summary of the trained model and analysis the result\n",
    "print(logreg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model doesnt work well, so we train the model with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9856</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9857</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9858</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9859</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9860</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9861</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9862</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9863</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9864</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9865</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9866</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9867</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9868</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9869</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9870</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9871</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9872</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9873</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9874</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9875</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9876</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9877</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9878</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9879</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9880</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9881</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9882</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9883</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9884</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9885</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9886 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Intercept\n",
       "0           1.0\n",
       "1           1.0\n",
       "2           1.0\n",
       "3           1.0\n",
       "4           1.0\n",
       "5           1.0\n",
       "6           1.0\n",
       "7           1.0\n",
       "8           1.0\n",
       "9           1.0\n",
       "10          1.0\n",
       "11          1.0\n",
       "12          1.0\n",
       "13          1.0\n",
       "14          1.0\n",
       "15          1.0\n",
       "16          1.0\n",
       "17          1.0\n",
       "18          1.0\n",
       "19          1.0\n",
       "20          1.0\n",
       "21          1.0\n",
       "22          1.0\n",
       "23          1.0\n",
       "24          1.0\n",
       "25          1.0\n",
       "26          1.0\n",
       "27          1.0\n",
       "28          1.0\n",
       "29          1.0\n",
       "...         ...\n",
       "9856        1.0\n",
       "9857        1.0\n",
       "9858        1.0\n",
       "9859        1.0\n",
       "9860        1.0\n",
       "9861        1.0\n",
       "9862        1.0\n",
       "9863        1.0\n",
       "9864        1.0\n",
       "9865        1.0\n",
       "9866        1.0\n",
       "9867        1.0\n",
       "9868        1.0\n",
       "9869        1.0\n",
       "9870        1.0\n",
       "9871        1.0\n",
       "9872        1.0\n",
       "9873        1.0\n",
       "9874        1.0\n",
       "9875        1.0\n",
       "9876        1.0\n",
       "9877        1.0\n",
       "9878        1.0\n",
       "9879        1.0\n",
       "9880        1.0\n",
       "9881        1.0\n",
       "9882        1.0\n",
       "9883        1.0\n",
       "9884        1.0\n",
       "9885        1.0\n",
       "\n",
       "[9886 rows x 1 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intercept = pd.DataFrame({'Intercept': np.ones(9886)})\n",
    "intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive features:\n",
      "       Intercept  SellerFeedbackRating  ListingPrice  ShippingPrice  \\\n",
      "0           1.0              0.000000      0.000000       0.000000   \n",
      "1           1.0              0.950000      0.235077       0.000000   \n",
      "2           1.0              0.980000      0.113752       0.466311   \n",
      "3           1.0              0.950000      0.092270       0.778519   \n",
      "4           1.0              0.940000      0.272583       0.599733   \n",
      "5           1.0              0.670000      0.399718       0.465644   \n",
      "6           1.0              1.000000      0.540412       0.000000   \n",
      "7           1.0              0.960000      0.459236       0.665777   \n",
      "8           1.0              0.910000      0.558725       0.333556   \n",
      "9           1.0              0.960000      0.480190       0.665777   \n",
      "10          1.0              0.960000      0.688854       0.000000   \n",
      "11          1.0              0.960000      0.524212       0.863909   \n",
      "12          1.0              0.940000      0.802606       0.000000   \n",
      "13          1.0              1.000000      0.752069       0.369580   \n",
      "14          1.0              0.940000      0.655925       1.000000   \n",
      "15          1.0              0.960000      0.814580       0.466978   \n",
      "16          1.0              1.000000      0.914422       0.089393   \n",
      "17          1.0              1.000000      0.990843       0.000000   \n",
      "18          1.0              1.000000      1.000000       0.000000   \n",
      "19          1.0              0.989899      0.015816       0.217796   \n",
      "20          1.0              0.969697      0.053887       0.000000   \n",
      "21          1.0              0.000000      0.058109       0.000000   \n",
      "22          1.0              0.969697      0.025680       0.197780   \n",
      "23          1.0              0.969697      0.029383       0.197780   \n",
      "24          1.0              0.939394      0.000000       0.429845   \n",
      "25          1.0              0.959596      0.040424       0.224138   \n",
      "26          1.0              0.919192      0.069461       0.099088   \n",
      "27          1.0              0.919192      0.080916       0.099088   \n",
      "28          1.0              0.888889      0.099121       0.261395   \n",
      "29          1.0              0.969697      0.121340       0.138724   \n",
      "...         ...                   ...           ...            ...   \n",
      "9856        1.0              1.000000      0.061013       0.296529   \n",
      "9857        1.0              0.960000      0.149015       0.222844   \n",
      "9858        1.0              0.910000      0.254797       0.111982   \n",
      "9859        1.0              0.930000      0.074188       0.475924   \n",
      "9860        1.0              1.000000      0.408416       0.154311   \n",
      "9861        1.0              0.910000      0.468790       0.167973   \n",
      "9862        1.0              0.880000      0.520594       0.265174   \n",
      "9863        1.0              0.960000      0.698516       0.134378   \n",
      "9864        1.0              0.000000      0.635201       0.341769   \n",
      "9865        1.0              0.980000      1.000000       0.000000   \n",
      "9866        1.0              0.960000      0.697109       1.000000   \n",
      "9867        1.0              0.950000      0.000000       0.251288   \n",
      "9868        1.0              0.980000      0.050013       0.178947   \n",
      "9869        1.0              0.840000      0.037350       0.201344   \n",
      "9870        1.0              0.000000      0.160527       0.000000   \n",
      "9871        1.0              0.960000      0.161678       0.000000   \n",
      "9872        1.0              0.960000      0.039908       0.223516   \n",
      "9873        1.0              0.940000      0.178946       0.000000   \n",
      "9874        1.0              1.000000      0.182144       0.000000   \n",
      "9875        1.0              0.960000      0.058199       0.223516   \n",
      "9876        1.0              1.000000      0.061013       0.296529   \n",
      "9877        1.0              0.960000      0.149015       0.222844   \n",
      "9878        1.0              0.910000      0.254797       0.111982   \n",
      "9879        1.0              0.930000      0.074188       0.475924   \n",
      "9880        1.0              1.000000      0.408416       0.154311   \n",
      "9881        1.0              0.910000      0.468790       0.167973   \n",
      "9882        1.0              0.880000      0.520594       0.265174   \n",
      "9883        1.0              0.960000      0.698516       0.134378   \n",
      "9884        1.0              0.000000      0.635201       0.341769   \n",
      "9885        1.0              0.980000      1.000000       0.000000   \n",
      "\n",
      "      ShippingTime_maxHours  IsFulfilledByAmazon  IsFeaturedMerchant  \n",
      "0                  0.000000                    1                   1  \n",
      "1                  0.097429                    0                   1  \n",
      "2                  0.011781                    0                   1  \n",
      "3                  0.104740                    0                   1  \n",
      "4                  0.002482                    0                   0  \n",
      "5                  0.000152                    0                   0  \n",
      "6                  0.115710                    0                   1  \n",
      "7                  0.043216                    0                   1  \n",
      "8                  0.242723                    0                   1  \n",
      "9                  0.043216                    0                   1  \n",
      "10                 0.851376                    0                   1  \n",
      "11                 0.171173                    0                   1  \n",
      "12                 0.004208                    0                   1  \n",
      "13                 0.000075                    0                   1  \n",
      "14                 0.007967                    0                   1  \n",
      "15                 0.999976                    0                   1  \n",
      "16                 0.000880                    0                   1  \n",
      "17                 0.000503                    0                   1  \n",
      "18                 0.000654                    0                   1  \n",
      "19                 0.081159                    0                   0  \n",
      "20                 0.851376                    0                   1  \n",
      "21                 0.000000                    1                   1  \n",
      "22                 0.043216                    0                   1  \n",
      "23                 0.043216                    0                   1  \n",
      "24                 0.058962                    0                   1  \n",
      "25                 0.104740                    0                   1  \n",
      "26                 0.242723                    0                   1  \n",
      "27                 0.242723                    0                   1  \n",
      "28                 0.187051                    0                   1  \n",
      "29                 0.999976                    0                   1  \n",
      "...                     ...                  ...                 ...  \n",
      "9856               0.009657                    0                   1  \n",
      "9857               0.171173                    0                   1  \n",
      "9858               0.242723                    0                   1  \n",
      "9859               0.058962                    0                   1  \n",
      "9860               0.000050                    0                   1  \n",
      "9861               0.273024                    0                   1  \n",
      "9862               0.187051                    0                   1  \n",
      "9863               1.000000                    0                   1  \n",
      "9864               0.000000                    0                   1  \n",
      "9865               0.035638                    0                   1  \n",
      "9866               0.004442                    0                   1  \n",
      "9867               0.104764                    0                   1  \n",
      "9868               0.081159                    0                   0  \n",
      "9869               0.003401                    0                   0  \n",
      "9870               0.000000                    1                   1  \n",
      "9871               0.851569                    0                   1  \n",
      "9872               0.043216                    0                   1  \n",
      "9873               0.004208                    0                   1  \n",
      "9874               0.115710                    0                   1  \n",
      "9875               0.043216                    0                   1  \n",
      "9876               0.009657                    0                   1  \n",
      "9877               0.171173                    0                   1  \n",
      "9878               0.242723                    0                   1  \n",
      "9879               0.058962                    0                   1  \n",
      "9880               0.000050                    0                   1  \n",
      "9881               0.273024                    0                   1  \n",
      "9882               0.187051                    0                   1  \n",
      "9883               1.000000                    0                   1  \n",
      "9884               0.000000                    0                   1  \n",
      "9885               0.035638                    0                   1  \n",
      "\n",
      "[9886 rows x 7 columns]\n",
      "\n",
      "Target feature:\n",
      " 0       0\n",
      "1       1\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      0\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      0\n",
      "18      0\n",
      "19      0\n",
      "20      1\n",
      "21      0\n",
      "22      0\n",
      "23      0\n",
      "24      0\n",
      "25      0\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "       ..\n",
      "9856    0\n",
      "9857    0\n",
      "9858    0\n",
      "9859    0\n",
      "9860    0\n",
      "9861    0\n",
      "9862    0\n",
      "9863    0\n",
      "9864    0\n",
      "9865    0\n",
      "9866    0\n",
      "9867    0\n",
      "9868    0\n",
      "9869    0\n",
      "9870    1\n",
      "9871    0\n",
      "9872    0\n",
      "9873    0\n",
      "9874    0\n",
      "9875    0\n",
      "9876    0\n",
      "9877    0\n",
      "9878    0\n",
      "9879    0\n",
      "9880    0\n",
      "9881    0\n",
      "9882    0\n",
      "9883    0\n",
      "9884    0\n",
      "9885    0\n",
      "Name: IsWinner, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare the descriptive features\n",
    "X = pd.concat([intercept, df[['SellerFeedbackRating','ListingPrice','ShippingPrice','ShippingTime_maxHours','IsFulfilledByAmazon','IsFeaturedMerchant']]], axis=1)\n",
    "y = df.IsWinner \n",
    "# Coeficients: [[-1.38721298 -1.0184894  -7.53957196 -3.08383918 -2.46540224  1.54274289 3.43549741]]\n",
    "print(\"Descriptive features:\\n\", X)\n",
    "print(\"\\nTarget feature:\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a model using logistic regression from scikit-learn.\n",
    "# Use only the descriptive feature Size.\n",
    "logreg = LogisticRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.951952255715\n",
      "Confusion matrix: \n",
      " [[9237  102]\n",
      " [ 373  174]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.97      9339\n",
      "          1       0.63      0.32      0.42       547\n",
      "\n",
      "avg / total       0.94      0.95      0.94      9886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the valuation of the model \n",
    "predictions = logreg.predict(X)\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y, predictions))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y, predictions))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the quality of the model on the training set.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Classification report we could see that the model is really good for our data frame. The precision is more than 90%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Print the coefficients learned by the model and discuss their statistical significance as well as their role in the model (e.g., interpret the model).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficients: \n",
      " [[-1.38721298 -1.0184894  -7.53957196 -3.08383918 -2.46540224  1.54274289\n",
      "   3.43549741]]\n"
     ]
    }
   ],
   "source": [
    "# Examine the estimated logistic regression coefficients.\n",
    "print(\"Coeficients: \\n\", logreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model, we could still see that ListingPrice is very important to the target feature, its weight is -7.53957196. \n",
    "SellerFeedbackRating has a very low weight:-1.0184894, it is very close to -1, so it shows that it doesn't hava\tasig nificant impact on\tthe\tmodel. The other features have significance impact on the model. \n",
    "\n",
    "Coeficients: [[-1.38721298 -1.0184894  -7.53957196 -3.08383918 -2.46540224  1.54274289 3.43549741]]                intercept,'SellerFeedbackRating','ListingPrice','ShippingPrice','ShippingTime_maxHours','IsFulfilledByAmazon','IsFeaturedMerchant'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Retrain the model using only the subset of features found to be statistically significant. Evaluate the quality of the model on the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive features:\n",
      "       Intercept  ListingPrice  ShippingPrice  ShippingTime_maxHours  \\\n",
      "0           1.0      0.000000       0.000000                  1.000   \n",
      "1           1.0      0.235077       0.000000                  0.025   \n",
      "2           1.0      0.113752       0.466311                  0.000   \n",
      "3           1.0      0.092270       0.778519                  0.000   \n",
      "4           1.0      0.272583       0.599733                  0.000   \n",
      "5           1.0      0.399718       0.465644                  0.000   \n",
      "6           1.0      0.540412       0.000000                  0.000   \n",
      "7           1.0      0.459236       0.665777                  0.075   \n",
      "8           1.0      0.558725       0.333556                  0.000   \n",
      "9           1.0      0.480190       0.665777                  0.075   \n",
      "10          1.0      0.688854       0.000000                  0.075   \n",
      "11          1.0      0.524212       0.863909                  0.000   \n",
      "12          1.0      0.802606       0.000000                  0.000   \n",
      "13          1.0      0.752069       0.369580                  0.000   \n",
      "14          1.0      0.655925       1.000000                  0.000   \n",
      "15          1.0      0.814580       0.466978                  0.000   \n",
      "16          1.0      0.914422       0.089393                  0.075   \n",
      "17          1.0      0.990843       0.000000                  0.050   \n",
      "18          1.0      1.000000       0.000000                  0.050   \n",
      "19          1.0      0.015816       0.217796                  0.000   \n",
      "20          1.0      0.053887       0.000000                  0.075   \n",
      "21          1.0      0.058109       0.000000                  1.000   \n",
      "22          1.0      0.025680       0.197780                  0.075   \n",
      "23          1.0      0.029383       0.197780                  0.075   \n",
      "24          1.0      0.000000       0.429845                  0.000   \n",
      "25          1.0      0.040424       0.224138                  0.000   \n",
      "26          1.0      0.069461       0.099088                  0.000   \n",
      "27          1.0      0.080916       0.099088                  0.000   \n",
      "28          1.0      0.099121       0.261395                  0.075   \n",
      "29          1.0      0.121340       0.138724                  0.000   \n",
      "...         ...           ...            ...                    ...   \n",
      "9856        1.0      0.061013       0.296529                  0.400   \n",
      "9857        1.0      0.149015       0.222844                  0.400   \n",
      "9858        1.0      0.254797       0.111982                  0.400   \n",
      "9859        1.0      0.074188       0.475924                  0.400   \n",
      "9860        1.0      0.408416       0.154311                  0.400   \n",
      "9861        1.0      0.468790       0.167973                  0.800   \n",
      "9862        1.0      0.520594       0.265174                  1.000   \n",
      "9863        1.0      0.698516       0.134378                  0.400   \n",
      "9864        1.0      0.635201       0.341769                  0.400   \n",
      "9865        1.0      1.000000       0.000000                  0.400   \n",
      "9866        1.0      0.697109       1.000000                  1.000   \n",
      "9867        1.0      0.000000       0.251288                  0.400   \n",
      "9868        1.0      0.050013       0.178947                  0.600   \n",
      "9869        1.0      0.037350       0.201344                  0.400   \n",
      "9870        1.0      0.160527       0.000000                  0.000   \n",
      "9871        1.0      0.161678       0.000000                  1.000   \n",
      "9872        1.0      0.039908       0.223516                  1.000   \n",
      "9873        1.0      0.178946       0.000000                  0.400   \n",
      "9874        1.0      0.182144       0.000000                  0.400   \n",
      "9875        1.0      0.058199       0.223516                  1.000   \n",
      "9876        1.0      0.061013       0.296529                  0.400   \n",
      "9877        1.0      0.149015       0.222844                  0.400   \n",
      "9878        1.0      0.254797       0.111982                  0.400   \n",
      "9879        1.0      0.074188       0.475924                  0.400   \n",
      "9880        1.0      0.408416       0.154311                  0.400   \n",
      "9881        1.0      0.468790       0.167973                  0.800   \n",
      "9882        1.0      0.520594       0.265174                  1.000   \n",
      "9883        1.0      0.698516       0.134378                  0.400   \n",
      "9884        1.0      0.635201       0.341769                  0.400   \n",
      "9885        1.0      1.000000       0.000000                  0.400   \n",
      "\n",
      "      IsFulfilledByAmazon  IsFeaturedMerchant  \n",
      "0                       1                   1  \n",
      "1                       0                   1  \n",
      "2                       0                   1  \n",
      "3                       0                   1  \n",
      "4                       0                   0  \n",
      "5                       0                   0  \n",
      "6                       0                   1  \n",
      "7                       0                   1  \n",
      "8                       0                   1  \n",
      "9                       0                   1  \n",
      "10                      0                   1  \n",
      "11                      0                   1  \n",
      "12                      0                   1  \n",
      "13                      0                   1  \n",
      "14                      0                   1  \n",
      "15                      0                   1  \n",
      "16                      0                   1  \n",
      "17                      0                   1  \n",
      "18                      0                   1  \n",
      "19                      0                   0  \n",
      "20                      0                   1  \n",
      "21                      1                   1  \n",
      "22                      0                   1  \n",
      "23                      0                   1  \n",
      "24                      0                   1  \n",
      "25                      0                   1  \n",
      "26                      0                   1  \n",
      "27                      0                   1  \n",
      "28                      0                   1  \n",
      "29                      0                   1  \n",
      "...                   ...                 ...  \n",
      "9856                    0                   1  \n",
      "9857                    0                   1  \n",
      "9858                    0                   1  \n",
      "9859                    0                   1  \n",
      "9860                    0                   1  \n",
      "9861                    0                   1  \n",
      "9862                    0                   1  \n",
      "9863                    0                   1  \n",
      "9864                    0                   1  \n",
      "9865                    0                   1  \n",
      "9866                    0                   1  \n",
      "9867                    0                   1  \n",
      "9868                    0                   0  \n",
      "9869                    0                   0  \n",
      "9870                    1                   1  \n",
      "9871                    0                   1  \n",
      "9872                    0                   1  \n",
      "9873                    0                   1  \n",
      "9874                    0                   1  \n",
      "9875                    0                   1  \n",
      "9876                    0                   1  \n",
      "9877                    0                   1  \n",
      "9878                    0                   1  \n",
      "9879                    0                   1  \n",
      "9880                    0                   1  \n",
      "9881                    0                   1  \n",
      "9882                    0                   1  \n",
      "9883                    0                   1  \n",
      "9884                    0                   1  \n",
      "9885                    0                   1  \n",
      "\n",
      "[9886 rows x 6 columns]\n",
      "\n",
      "Target feature:\n",
      " 0       0\n",
      "1       1\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      0\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      0\n",
      "18      0\n",
      "19      0\n",
      "20      1\n",
      "21      0\n",
      "22      0\n",
      "23      0\n",
      "24      0\n",
      "25      0\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "       ..\n",
      "9856    0\n",
      "9857    0\n",
      "9858    0\n",
      "9859    0\n",
      "9860    0\n",
      "9861    0\n",
      "9862    0\n",
      "9863    0\n",
      "9864    0\n",
      "9865    0\n",
      "9866    0\n",
      "9867    0\n",
      "9868    0\n",
      "9869    0\n",
      "9870    1\n",
      "9871    0\n",
      "9872    0\n",
      "9873    0\n",
      "9874    0\n",
      "9875    0\n",
      "9876    0\n",
      "9877    0\n",
      "9878    0\n",
      "9879    0\n",
      "9880    0\n",
      "9881    0\n",
      "9882    0\n",
      "9883    0\n",
      "9884    0\n",
      "9885    0\n",
      "Name: IsWinner, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare the descriptive features\n",
    "X = pd.concat([intercept, df[['ListingPrice','ShippingPrice','ShippingTime_maxHours','IsFulfilledByAmazon','IsFeaturedMerchant']]], axis=1)\n",
    "y = df.IsWinner \n",
    "print(\"Descriptive features:\\n\", X)\n",
    "print(\"\\nTarget feature:\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train a model using logistic regression from scikit-learn.\n",
    "# Use only the descriptive feature Size.\n",
    "logreg = LogisticRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficients: \n",
      " [[-1.74424582 -7.54278132 -3.03977395 -2.46462883  2.3773813   3.26389958]]\n"
     ]
    }
   ],
   "source": [
    "# Examine the estimated logistic regression coefficients.\n",
    "print(\"Coeficients: \\n\", logreg.coef_)\n",
    "#pd.DataFrame(zip(X[['Size']], np.transpose(logreg.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.964090633219\n",
      "Confusion matrix: \n",
      " [[9328   11]\n",
      " [ 344  203]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      9339\n",
      "          1       0.95      0.37      0.53       547\n",
      "\n",
      "avg / total       0.96      0.96      0.96      9886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some more evaluation metrics.\n",
    "predictions = logreg.predict(X)\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y, predictions))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y, predictions))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Using the better model (as per evaluation on training set), print the predicted target feature value for all the examples in the training set. Print the predicted class for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>ListingPrice</th>\n",
       "      <th>ShippingPrice</th>\n",
       "      <th>ShippingTime_maxHours</th>\n",
       "      <th>IsFulfilledByAmazon</th>\n",
       "      <th>IsFeaturedMerchant</th>\n",
       "      <th>IsWinner</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.235077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.113752</td>\n",
       "      <td>0.466311</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.092270</td>\n",
       "      <td>0.778519</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.272583</td>\n",
       "      <td>0.599733</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.399718</td>\n",
       "      <td>0.465644</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.459236</td>\n",
       "      <td>0.665777</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.558725</td>\n",
       "      <td>0.333556</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.480190</td>\n",
       "      <td>0.665777</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.688854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.524212</td>\n",
       "      <td>0.863909</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.802606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.752069</td>\n",
       "      <td>0.369580</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.655925</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.814580</td>\n",
       "      <td>0.466978</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914422</td>\n",
       "      <td>0.089393</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015816</td>\n",
       "      <td>0.217796</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.053887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.058109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.025680</td>\n",
       "      <td>0.197780</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.029383</td>\n",
       "      <td>0.197780</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.429845</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.040424</td>\n",
       "      <td>0.224138</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.069461</td>\n",
       "      <td>0.099088</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.080916</td>\n",
       "      <td>0.099088</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.099121</td>\n",
       "      <td>0.261395</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.121340</td>\n",
       "      <td>0.138724</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9856</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.061013</td>\n",
       "      <td>0.296529</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9857</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.222844</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9858</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.254797</td>\n",
       "      <td>0.111982</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9859</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.074188</td>\n",
       "      <td>0.475924</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9860</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.408416</td>\n",
       "      <td>0.154311</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9861</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468790</td>\n",
       "      <td>0.167973</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9862</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.520594</td>\n",
       "      <td>0.265174</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9863</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.698516</td>\n",
       "      <td>0.134378</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9864</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.635201</td>\n",
       "      <td>0.341769</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9865</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9866</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.697109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9867</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251288</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9868</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.050013</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9869</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.037350</td>\n",
       "      <td>0.201344</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9870</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.160527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9871</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.161678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9872</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.039908</td>\n",
       "      <td>0.223516</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9873</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.178946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9874</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.182144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9875</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.058199</td>\n",
       "      <td>0.223516</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9876</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.061013</td>\n",
       "      <td>0.296529</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9877</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.222844</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9878</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.254797</td>\n",
       "      <td>0.111982</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9879</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.074188</td>\n",
       "      <td>0.475924</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9880</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.408416</td>\n",
       "      <td>0.154311</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9881</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468790</td>\n",
       "      <td>0.167973</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9882</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.520594</td>\n",
       "      <td>0.265174</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9883</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.698516</td>\n",
       "      <td>0.134378</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9884</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.635201</td>\n",
       "      <td>0.341769</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9885</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9886 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Intercept  ListingPrice  ShippingPrice  ShippingTime_maxHours  \\\n",
       "0           1.0      0.000000       0.000000                  1.000   \n",
       "1           1.0      0.235077       0.000000                  0.025   \n",
       "2           1.0      0.113752       0.466311                  0.000   \n",
       "3           1.0      0.092270       0.778519                  0.000   \n",
       "4           1.0      0.272583       0.599733                  0.000   \n",
       "5           1.0      0.399718       0.465644                  0.000   \n",
       "6           1.0      0.540412       0.000000                  0.000   \n",
       "7           1.0      0.459236       0.665777                  0.075   \n",
       "8           1.0      0.558725       0.333556                  0.000   \n",
       "9           1.0      0.480190       0.665777                  0.075   \n",
       "10          1.0      0.688854       0.000000                  0.075   \n",
       "11          1.0      0.524212       0.863909                  0.000   \n",
       "12          1.0      0.802606       0.000000                  0.000   \n",
       "13          1.0      0.752069       0.369580                  0.000   \n",
       "14          1.0      0.655925       1.000000                  0.000   \n",
       "15          1.0      0.814580       0.466978                  0.000   \n",
       "16          1.0      0.914422       0.089393                  0.075   \n",
       "17          1.0      0.990843       0.000000                  0.050   \n",
       "18          1.0      1.000000       0.000000                  0.050   \n",
       "19          1.0      0.015816       0.217796                  0.000   \n",
       "20          1.0      0.053887       0.000000                  0.075   \n",
       "21          1.0      0.058109       0.000000                  1.000   \n",
       "22          1.0      0.025680       0.197780                  0.075   \n",
       "23          1.0      0.029383       0.197780                  0.075   \n",
       "24          1.0      0.000000       0.429845                  0.000   \n",
       "25          1.0      0.040424       0.224138                  0.000   \n",
       "26          1.0      0.069461       0.099088                  0.000   \n",
       "27          1.0      0.080916       0.099088                  0.000   \n",
       "28          1.0      0.099121       0.261395                  0.075   \n",
       "29          1.0      0.121340       0.138724                  0.000   \n",
       "...         ...           ...            ...                    ...   \n",
       "9856        1.0      0.061013       0.296529                  0.400   \n",
       "9857        1.0      0.149015       0.222844                  0.400   \n",
       "9858        1.0      0.254797       0.111982                  0.400   \n",
       "9859        1.0      0.074188       0.475924                  0.400   \n",
       "9860        1.0      0.408416       0.154311                  0.400   \n",
       "9861        1.0      0.468790       0.167973                  0.800   \n",
       "9862        1.0      0.520594       0.265174                  1.000   \n",
       "9863        1.0      0.698516       0.134378                  0.400   \n",
       "9864        1.0      0.635201       0.341769                  0.400   \n",
       "9865        1.0      1.000000       0.000000                  0.400   \n",
       "9866        1.0      0.697109       1.000000                  1.000   \n",
       "9867        1.0      0.000000       0.251288                  0.400   \n",
       "9868        1.0      0.050013       0.178947                  0.600   \n",
       "9869        1.0      0.037350       0.201344                  0.400   \n",
       "9870        1.0      0.160527       0.000000                  0.000   \n",
       "9871        1.0      0.161678       0.000000                  1.000   \n",
       "9872        1.0      0.039908       0.223516                  1.000   \n",
       "9873        1.0      0.178946       0.000000                  0.400   \n",
       "9874        1.0      0.182144       0.000000                  0.400   \n",
       "9875        1.0      0.058199       0.223516                  1.000   \n",
       "9876        1.0      0.061013       0.296529                  0.400   \n",
       "9877        1.0      0.149015       0.222844                  0.400   \n",
       "9878        1.0      0.254797       0.111982                  0.400   \n",
       "9879        1.0      0.074188       0.475924                  0.400   \n",
       "9880        1.0      0.408416       0.154311                  0.400   \n",
       "9881        1.0      0.468790       0.167973                  0.800   \n",
       "9882        1.0      0.520594       0.265174                  1.000   \n",
       "9883        1.0      0.698516       0.134378                  0.400   \n",
       "9884        1.0      0.635201       0.341769                  0.400   \n",
       "9885        1.0      1.000000       0.000000                  0.400   \n",
       "\n",
       "      IsFulfilledByAmazon  IsFeaturedMerchant  IsWinner  predicted  \n",
       "0                       1                   1         0          0  \n",
       "1                       0                   1         1          0  \n",
       "2                       0                   1         0          0  \n",
       "3                       0                   1         0          0  \n",
       "4                       0                   0         0          0  \n",
       "5                       0                   0         0          0  \n",
       "6                       0                   1         0          0  \n",
       "7                       0                   1         0          0  \n",
       "8                       0                   1         0          0  \n",
       "9                       0                   1         0          0  \n",
       "10                      0                   1         0          0  \n",
       "11                      0                   1         0          0  \n",
       "12                      0                   1         0          0  \n",
       "13                      0                   1         0          0  \n",
       "14                      0                   1         0          0  \n",
       "15                      0                   1         0          0  \n",
       "16                      0                   1         0          0  \n",
       "17                      0                   1         0          0  \n",
       "18                      0                   1         0          0  \n",
       "19                      0                   0         0          0  \n",
       "20                      0                   1         1          0  \n",
       "21                      1                   1         0          0  \n",
       "22                      0                   1         0          0  \n",
       "23                      0                   1         0          0  \n",
       "24                      0                   1         0          0  \n",
       "25                      0                   1         0          0  \n",
       "26                      0                   1         0          0  \n",
       "27                      0                   1         0          0  \n",
       "28                      0                   1         0          0  \n",
       "29                      0                   1         0          0  \n",
       "...                   ...                 ...       ...        ...  \n",
       "9856                    0                   1         0          0  \n",
       "9857                    0                   1         0          0  \n",
       "9858                    0                   1         0          0  \n",
       "9859                    0                   1         0          0  \n",
       "9860                    0                   1         0          0  \n",
       "9861                    0                   1         0          0  \n",
       "9862                    0                   1         0          0  \n",
       "9863                    0                   1         0          0  \n",
       "9864                    0                   1         0          0  \n",
       "9865                    0                   1         0          0  \n",
       "9866                    0                   1         0          0  \n",
       "9867                    0                   1         0          0  \n",
       "9868                    0                   0         0          0  \n",
       "9869                    0                   0         0          0  \n",
       "9870                    1                   1         1          1  \n",
       "9871                    0                   1         0          0  \n",
       "9872                    0                   1         0          0  \n",
       "9873                    0                   1         0          0  \n",
       "9874                    0                   1         0          0  \n",
       "9875                    0                   1         0          0  \n",
       "9876                    0                   1         0          0  \n",
       "9877                    0                   1         0          0  \n",
       "9878                    0                   1         0          0  \n",
       "9879                    0                   1         0          0  \n",
       "9880                    0                   1         0          0  \n",
       "9881                    0                   1         0          0  \n",
       "9882                    0                   1         0          0  \n",
       "9883                    0                   1         0          0  \n",
       "9884                    0                   1         0          0  \n",
       "9885                    0                   1         0          0  \n",
       "\n",
       "[9886 rows x 8 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = pd.DataFrame({'predicted': predictions})\n",
    "Result=pd.concat([X,y,predicted], axis=1)\n",
    "Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Predictive Modeling: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1  Train a random forest model to predict the target feature IsWinner, using the descriptive features selected in exercise (1). Evaluate the quality of the model on the training set.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive features:\n",
      "       SellerFeedbackRating  ListingPrice  ShippingPrice  \\\n",
      "0                 0.000000      0.000000       0.000000   \n",
      "1                 0.950000      0.235077       0.000000   \n",
      "2                 0.980000      0.113752       0.466311   \n",
      "3                 0.950000      0.092270       0.778519   \n",
      "4                 0.940000      0.272583       0.599733   \n",
      "5                 0.670000      0.399718       0.465644   \n",
      "6                 1.000000      0.540412       0.000000   \n",
      "7                 0.960000      0.459236       0.665777   \n",
      "8                 0.910000      0.558725       0.333556   \n",
      "9                 0.960000      0.480190       0.665777   \n",
      "10                0.960000      0.688854       0.000000   \n",
      "11                0.960000      0.524212       0.863909   \n",
      "12                0.940000      0.802606       0.000000   \n",
      "13                1.000000      0.752069       0.369580   \n",
      "14                0.940000      0.655925       1.000000   \n",
      "15                0.960000      0.814580       0.466978   \n",
      "16                1.000000      0.914422       0.089393   \n",
      "17                1.000000      0.990843       0.000000   \n",
      "18                1.000000      1.000000       0.000000   \n",
      "19                0.989899      0.015816       0.217796   \n",
      "20                0.969697      0.053887       0.000000   \n",
      "21                0.000000      0.058109       0.000000   \n",
      "22                0.969697      0.025680       0.197780   \n",
      "23                0.969697      0.029383       0.197780   \n",
      "24                0.939394      0.000000       0.429845   \n",
      "25                0.959596      0.040424       0.224138   \n",
      "26                0.919192      0.069461       0.099088   \n",
      "27                0.919192      0.080916       0.099088   \n",
      "28                0.888889      0.099121       0.261395   \n",
      "29                0.969697      0.121340       0.138724   \n",
      "...                    ...           ...            ...   \n",
      "9856              1.000000      0.061013       0.296529   \n",
      "9857              0.960000      0.149015       0.222844   \n",
      "9858              0.910000      0.254797       0.111982   \n",
      "9859              0.930000      0.074188       0.475924   \n",
      "9860              1.000000      0.408416       0.154311   \n",
      "9861              0.910000      0.468790       0.167973   \n",
      "9862              0.880000      0.520594       0.265174   \n",
      "9863              0.960000      0.698516       0.134378   \n",
      "9864              0.000000      0.635201       0.341769   \n",
      "9865              0.980000      1.000000       0.000000   \n",
      "9866              0.960000      0.697109       1.000000   \n",
      "9867              0.950000      0.000000       0.251288   \n",
      "9868              0.980000      0.050013       0.178947   \n",
      "9869              0.840000      0.037350       0.201344   \n",
      "9870              0.000000      0.160527       0.000000   \n",
      "9871              0.960000      0.161678       0.000000   \n",
      "9872              0.960000      0.039908       0.223516   \n",
      "9873              0.940000      0.178946       0.000000   \n",
      "9874              1.000000      0.182144       0.000000   \n",
      "9875              0.960000      0.058199       0.223516   \n",
      "9876              1.000000      0.061013       0.296529   \n",
      "9877              0.960000      0.149015       0.222844   \n",
      "9878              0.910000      0.254797       0.111982   \n",
      "9879              0.930000      0.074188       0.475924   \n",
      "9880              1.000000      0.408416       0.154311   \n",
      "9881              0.910000      0.468790       0.167973   \n",
      "9882              0.880000      0.520594       0.265174   \n",
      "9883              0.960000      0.698516       0.134378   \n",
      "9884              0.000000      0.635201       0.341769   \n",
      "9885              0.980000      1.000000       0.000000   \n",
      "\n",
      "      ShippingTime_maxHours  IsFulfilledByAmazon  IsFeaturedMerchant  \n",
      "0                  0.000000                    1                   1  \n",
      "1                  0.097429                    0                   1  \n",
      "2                  0.011781                    0                   1  \n",
      "3                  0.104740                    0                   1  \n",
      "4                  0.002482                    0                   0  \n",
      "5                  0.000152                    0                   0  \n",
      "6                  0.115710                    0                   1  \n",
      "7                  0.043216                    0                   1  \n",
      "8                  0.242723                    0                   1  \n",
      "9                  0.043216                    0                   1  \n",
      "10                 0.851376                    0                   1  \n",
      "11                 0.171173                    0                   1  \n",
      "12                 0.004208                    0                   1  \n",
      "13                 0.000075                    0                   1  \n",
      "14                 0.007967                    0                   1  \n",
      "15                 0.999976                    0                   1  \n",
      "16                 0.000880                    0                   1  \n",
      "17                 0.000503                    0                   1  \n",
      "18                 0.000654                    0                   1  \n",
      "19                 0.081159                    0                   0  \n",
      "20                 0.851376                    0                   1  \n",
      "21                 0.000000                    1                   1  \n",
      "22                 0.043216                    0                   1  \n",
      "23                 0.043216                    0                   1  \n",
      "24                 0.058962                    0                   1  \n",
      "25                 0.104740                    0                   1  \n",
      "26                 0.242723                    0                   1  \n",
      "27                 0.242723                    0                   1  \n",
      "28                 0.187051                    0                   1  \n",
      "29                 0.999976                    0                   1  \n",
      "...                     ...                  ...                 ...  \n",
      "9856               0.009657                    0                   1  \n",
      "9857               0.171173                    0                   1  \n",
      "9858               0.242723                    0                   1  \n",
      "9859               0.058962                    0                   1  \n",
      "9860               0.000050                    0                   1  \n",
      "9861               0.273024                    0                   1  \n",
      "9862               0.187051                    0                   1  \n",
      "9863               1.000000                    0                   1  \n",
      "9864               0.000000                    0                   1  \n",
      "9865               0.035638                    0                   1  \n",
      "9866               0.004442                    0                   1  \n",
      "9867               0.104764                    0                   1  \n",
      "9868               0.081159                    0                   0  \n",
      "9869               0.003401                    0                   0  \n",
      "9870               0.000000                    1                   1  \n",
      "9871               0.851569                    0                   1  \n",
      "9872               0.043216                    0                   1  \n",
      "9873               0.004208                    0                   1  \n",
      "9874               0.115710                    0                   1  \n",
      "9875               0.043216                    0                   1  \n",
      "9876               0.009657                    0                   1  \n",
      "9877               0.171173                    0                   1  \n",
      "9878               0.242723                    0                   1  \n",
      "9879               0.058962                    0                   1  \n",
      "9880               0.000050                    0                   1  \n",
      "9881               0.273024                    0                   1  \n",
      "9882               0.187051                    0                   1  \n",
      "9883               1.000000                    0                   1  \n",
      "9884               0.000000                    0                   1  \n",
      "9885               0.035638                    0                   1  \n",
      "\n",
      "[9886 rows x 6 columns]\n",
      "\n",
      "Target feature:\n",
      " 0       0\n",
      "1       1\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      0\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      0\n",
      "18      0\n",
      "19      0\n",
      "20      1\n",
      "21      0\n",
      "22      0\n",
      "23      0\n",
      "24      0\n",
      "25      0\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "       ..\n",
      "9856    0\n",
      "9857    0\n",
      "9858    0\n",
      "9859    0\n",
      "9860    0\n",
      "9861    0\n",
      "9862    0\n",
      "9863    0\n",
      "9864    0\n",
      "9865    0\n",
      "9866    0\n",
      "9867    0\n",
      "9868    0\n",
      "9869    0\n",
      "9870    1\n",
      "9871    0\n",
      "9872    0\n",
      "9873    0\n",
      "9874    0\n",
      "9875    0\n",
      "9876    0\n",
      "9877    0\n",
      "9878    0\n",
      "9879    0\n",
      "9880    0\n",
      "9881    0\n",
      "9882    0\n",
      "9883    0\n",
      "9884    0\n",
      "9885    0\n",
      "Name: IsWinner, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare the descriptive features\n",
    "X = pd.concat([df[['SellerFeedbackRating','ListingPrice','ShippingPrice','ShippingTime_maxHours','IsFulfilledByAmazon','IsFeaturedMerchant']]], axis=1)\n",
    "y = df.IsWinner \n",
    "print(\"Descriptive features:\\n\", X)\n",
    "print(\"\\nTarget feature:\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=1, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "#Train a decision tree classifier model\n",
    "#Instantiate estimator, fit with training set\n",
    "# Train a classification tree with max_depth=6 on all data\n",
    "dtc = DecisionTreeClassifier(max_depth=6, random_state=1)\n",
    "dtc.fit(X, y)\n",
    "print(dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SellerFeedbackRating</td>\n",
       "      <td>0.034734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ListingPrice</td>\n",
       "      <td>0.232835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ShippingPrice</td>\n",
       "      <td>0.093527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ShippingTime_maxHours</td>\n",
       "      <td>0.054357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IsFulfilledByAmazon</td>\n",
       "      <td>0.539846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IsFeaturedMerchant</td>\n",
       "      <td>0.044701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature  importance\n",
       "0   SellerFeedbackRating    0.034734\n",
       "1           ListingPrice    0.232835\n",
       "2          ShippingPrice    0.093527\n",
       "3  ShippingTime_maxHours    0.054357\n",
       "4    IsFulfilledByAmazon    0.539846\n",
       "5     IsFeaturedMerchant    0.044701"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the importance of each feature based on the trained decision tree classifier\n",
    "pd.DataFrame({'feature': X.columns, 'importance': dtc.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.959437588509\n",
      "Confusion matrix: \n",
      " [[9212  127]\n",
      " [ 274  273]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98      9339\n",
      "          1       0.68      0.50      0.58       547\n",
      "\n",
      "avg / total       0.96      0.96      0.96      9886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions=dtc.predict(X)\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y, predictions))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y, predictions))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the quality of the model on the training set.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Classification report we could see that the model works better than linear regression model. The precision of IsWineer==1 is very high 98%. While the IsWinner ==0, the precision is lower than logistic regression. While the total result is 97%. So the model quality is good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Print the features ranked by random forest importance. Discuss your findings and choose a subset of features you find promising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SellerFeedbackRating</td>\n",
       "      <td>0.034734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ListingPrice</td>\n",
       "      <td>0.232835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ShippingPrice</td>\n",
       "      <td>0.093527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ShippingTime_maxHours</td>\n",
       "      <td>0.054357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IsFulfilledByAmazon</td>\n",
       "      <td>0.539846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IsFeaturedMerchant</td>\n",
       "      <td>0.044701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature  importance\n",
       "0   SellerFeedbackRating    0.034734\n",
       "1           ListingPrice    0.232835\n",
       "2          ShippingPrice    0.093527\n",
       "3  ShippingTime_maxHours    0.054357\n",
       "4    IsFulfilledByAmazon    0.539846\n",
       "5     IsFeaturedMerchant    0.044701"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the importance of each feature based on the trained decision tree classifier\n",
    "pd.DataFrame({'feature': X.columns, 'importance': dtc.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model, IsFulfilledByAmazon's importance is 0.410580 and it could change the target feature more than the other features. \n",
    "IsFeaturedMerchant and SellerFeedbackRating both have very low importance: \t0.045270 and  0.051060. The both are close to 0, it means that they are not important to our model. \n",
    "The other features have significance impact on the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Retrain the model using only the subset of features found to be promising. Evaluate the quality of the model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive features:\n",
      "       ListingPrice  ShippingPrice  ShippingTime_maxHours  IsFulfilledByAmazon  \\\n",
      "0         0.000000       0.000000               0.000000                    1   \n",
      "1         0.235077       0.000000               0.097429                    0   \n",
      "2         0.113752       0.466311               0.011781                    0   \n",
      "3         0.092270       0.778519               0.104740                    0   \n",
      "4         0.272583       0.599733               0.002482                    0   \n",
      "5         0.399718       0.465644               0.000152                    0   \n",
      "6         0.540412       0.000000               0.115710                    0   \n",
      "7         0.459236       0.665777               0.043216                    0   \n",
      "8         0.558725       0.333556               0.242723                    0   \n",
      "9         0.480190       0.665777               0.043216                    0   \n",
      "10        0.688854       0.000000               0.851376                    0   \n",
      "11        0.524212       0.863909               0.171173                    0   \n",
      "12        0.802606       0.000000               0.004208                    0   \n",
      "13        0.752069       0.369580               0.000075                    0   \n",
      "14        0.655925       1.000000               0.007967                    0   \n",
      "15        0.814580       0.466978               0.999976                    0   \n",
      "16        0.914422       0.089393               0.000880                    0   \n",
      "17        0.990843       0.000000               0.000503                    0   \n",
      "18        1.000000       0.000000               0.000654                    0   \n",
      "19        0.015816       0.217796               0.081159                    0   \n",
      "20        0.053887       0.000000               0.851376                    0   \n",
      "21        0.058109       0.000000               0.000000                    1   \n",
      "22        0.025680       0.197780               0.043216                    0   \n",
      "23        0.029383       0.197780               0.043216                    0   \n",
      "24        0.000000       0.429845               0.058962                    0   \n",
      "25        0.040424       0.224138               0.104740                    0   \n",
      "26        0.069461       0.099088               0.242723                    0   \n",
      "27        0.080916       0.099088               0.242723                    0   \n",
      "28        0.099121       0.261395               0.187051                    0   \n",
      "29        0.121340       0.138724               0.999976                    0   \n",
      "...            ...            ...                    ...                  ...   \n",
      "9856      0.061013       0.296529               0.009657                    0   \n",
      "9857      0.149015       0.222844               0.171173                    0   \n",
      "9858      0.254797       0.111982               0.242723                    0   \n",
      "9859      0.074188       0.475924               0.058962                    0   \n",
      "9860      0.408416       0.154311               0.000050                    0   \n",
      "9861      0.468790       0.167973               0.273024                    0   \n",
      "9862      0.520594       0.265174               0.187051                    0   \n",
      "9863      0.698516       0.134378               1.000000                    0   \n",
      "9864      0.635201       0.341769               0.000000                    0   \n",
      "9865      1.000000       0.000000               0.035638                    0   \n",
      "9866      0.697109       1.000000               0.004442                    0   \n",
      "9867      0.000000       0.251288               0.104764                    0   \n",
      "9868      0.050013       0.178947               0.081159                    0   \n",
      "9869      0.037350       0.201344               0.003401                    0   \n",
      "9870      0.160527       0.000000               0.000000                    1   \n",
      "9871      0.161678       0.000000               0.851569                    0   \n",
      "9872      0.039908       0.223516               0.043216                    0   \n",
      "9873      0.178946       0.000000               0.004208                    0   \n",
      "9874      0.182144       0.000000               0.115710                    0   \n",
      "9875      0.058199       0.223516               0.043216                    0   \n",
      "9876      0.061013       0.296529               0.009657                    0   \n",
      "9877      0.149015       0.222844               0.171173                    0   \n",
      "9878      0.254797       0.111982               0.242723                    0   \n",
      "9879      0.074188       0.475924               0.058962                    0   \n",
      "9880      0.408416       0.154311               0.000050                    0   \n",
      "9881      0.468790       0.167973               0.273024                    0   \n",
      "9882      0.520594       0.265174               0.187051                    0   \n",
      "9883      0.698516       0.134378               1.000000                    0   \n",
      "9884      0.635201       0.341769               0.000000                    0   \n",
      "9885      1.000000       0.000000               0.035638                    0   \n",
      "\n",
      "      Rating_score  \n",
      "0                0  \n",
      "1           387410  \n",
      "2            46844  \n",
      "3           416480  \n",
      "4             9870  \n",
      "5              603  \n",
      "6           460100  \n",
      "7           171840  \n",
      "8           965146  \n",
      "9           171840  \n",
      "10         3385344  \n",
      "11          680640  \n",
      "12           16732  \n",
      "13             300  \n",
      "14           31678  \n",
      "15         3976224  \n",
      "16            3500  \n",
      "17            2000  \n",
      "18            2600  \n",
      "19          322714  \n",
      "20         3385344  \n",
      "21               0  \n",
      "22          171840  \n",
      "23          171840  \n",
      "24          234453  \n",
      "25          416480  \n",
      "26          965146  \n",
      "27          965146  \n",
      "28          743776  \n",
      "29         3976224  \n",
      "...            ...  \n",
      "9856         38400  \n",
      "9857        680640  \n",
      "9858        965146  \n",
      "9859        234453  \n",
      "9860           200  \n",
      "9861       1085630  \n",
      "9862        743776  \n",
      "9863       3976320  \n",
      "9864             0  \n",
      "9865        141708  \n",
      "9866         17664  \n",
      "9867        416575  \n",
      "9868        322714  \n",
      "9869         13524  \n",
      "9870             0  \n",
      "9871       3386112  \n",
      "9872        171840  \n",
      "9873         16732  \n",
      "9874        460100  \n",
      "9875        171840  \n",
      "9876         38400  \n",
      "9877        680640  \n",
      "9878        965146  \n",
      "9879        234453  \n",
      "9880           200  \n",
      "9881       1085630  \n",
      "9882        743776  \n",
      "9883       3976320  \n",
      "9884             0  \n",
      "9885        141708  \n",
      "\n",
      "[9886 rows x 5 columns]\n",
      "\n",
      "Target feature:\n",
      " 0       0\n",
      "1       1\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      0\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      0\n",
      "18      0\n",
      "19      0\n",
      "20      1\n",
      "21      0\n",
      "22      0\n",
      "23      0\n",
      "24      0\n",
      "25      0\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "       ..\n",
      "9856    0\n",
      "9857    0\n",
      "9858    0\n",
      "9859    0\n",
      "9860    0\n",
      "9861    0\n",
      "9862    0\n",
      "9863    0\n",
      "9864    0\n",
      "9865    0\n",
      "9866    0\n",
      "9867    0\n",
      "9868    0\n",
      "9869    0\n",
      "9870    1\n",
      "9871    0\n",
      "9872    0\n",
      "9873    0\n",
      "9874    0\n",
      "9875    0\n",
      "9876    0\n",
      "9877    0\n",
      "9878    0\n",
      "9879    0\n",
      "9880    0\n",
      "9881    0\n",
      "9882    0\n",
      "9883    0\n",
      "9884    0\n",
      "9885    0\n",
      "Name: IsWinner, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare with new feature \n",
    "\n",
    "\n",
    "X = pd.concat([df[['ListingPrice','ShippingPrice','ShippingTime_maxHours','IsFulfilledByAmazon','Rating_score']]], axis=1)\n",
    "y = df.IsWinner \n",
    "print(\"Descriptive features:\\n\", X)\n",
    "print(\"\\nTarget feature:\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=1, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "#Train a decision tree classifier model\n",
    "#Instantiate estimator, fit with training set\n",
    "# Train a classification tree with max_depth=6 on all data\n",
    "dtc = DecisionTreeClassifier(max_depth=5, random_state=1)\n",
    "dtc.fit(X, y)\n",
    "print(dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ListingPrice</td>\n",
       "      <td>0.217453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ShippingPrice</td>\n",
       "      <td>0.093787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ShippingTime_maxHours</td>\n",
       "      <td>0.018997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IsFulfilledByAmazon</td>\n",
       "      <td>0.669764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rating_score</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature  importance\n",
       "0           ListingPrice    0.217453\n",
       "1          ShippingPrice    0.093787\n",
       "2  ShippingTime_maxHours    0.018997\n",
       "3    IsFulfilledByAmazon    0.669764\n",
       "4           Rating_score    0.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the importance of each feature based on the trained decision tree classifier\n",
    "pd.DataFrame({'feature': X.columns, 'importance': dtc.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=dtc.predict(X)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.954076471778\n",
      "Confusion matrix: \n",
      " [[9214  125]\n",
      " [ 329  218]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98      9339\n",
      "          1       0.64      0.40      0.49       547\n",
      "\n",
      "avg / total       0.95      0.95      0.95      9886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score(y, predictions))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y, predictions))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive features:\n",
      "       ListingPrice  ShippingPrice  ShippingTime_maxHours  IsFulfilledByAmazon\n",
      "0         0.000000       0.000000                  1.000                    1\n",
      "1         0.235077       0.000000                  0.025                    0\n",
      "2         0.113752       0.466311                  0.000                    0\n",
      "3         0.092270       0.778519                  0.000                    0\n",
      "4         0.272583       0.599733                  0.000                    0\n",
      "5         0.399718       0.465644                  0.000                    0\n",
      "6         0.540412       0.000000                  0.000                    0\n",
      "7         0.459236       0.665777                  0.075                    0\n",
      "8         0.558725       0.333556                  0.000                    0\n",
      "9         0.480190       0.665777                  0.075                    0\n",
      "10        0.688854       0.000000                  0.075                    0\n",
      "11        0.524212       0.863909                  0.000                    0\n",
      "12        0.802606       0.000000                  0.000                    0\n",
      "13        0.752069       0.369580                  0.000                    0\n",
      "14        0.655925       1.000000                  0.000                    0\n",
      "15        0.814580       0.466978                  0.000                    0\n",
      "16        0.914422       0.089393                  0.075                    0\n",
      "17        0.990843       0.000000                  0.050                    0\n",
      "18        1.000000       0.000000                  0.050                    0\n",
      "19        0.015816       0.217796                  0.000                    0\n",
      "20        0.053887       0.000000                  0.075                    0\n",
      "21        0.058109       0.000000                  1.000                    1\n",
      "22        0.025680       0.197780                  0.075                    0\n",
      "23        0.029383       0.197780                  0.075                    0\n",
      "24        0.000000       0.429845                  0.000                    0\n",
      "25        0.040424       0.224138                  0.000                    0\n",
      "26        0.069461       0.099088                  0.000                    0\n",
      "27        0.080916       0.099088                  0.000                    0\n",
      "28        0.099121       0.261395                  0.075                    0\n",
      "29        0.121340       0.138724                  0.000                    0\n",
      "...            ...            ...                    ...                  ...\n",
      "9856      0.061013       0.296529                  0.400                    0\n",
      "9857      0.149015       0.222844                  0.400                    0\n",
      "9858      0.254797       0.111982                  0.400                    0\n",
      "9859      0.074188       0.475924                  0.400                    0\n",
      "9860      0.408416       0.154311                  0.400                    0\n",
      "9861      0.468790       0.167973                  0.800                    0\n",
      "9862      0.520594       0.265174                  1.000                    0\n",
      "9863      0.698516       0.134378                  0.400                    0\n",
      "9864      0.635201       0.341769                  0.400                    0\n",
      "9865      1.000000       0.000000                  0.400                    0\n",
      "9866      0.697109       1.000000                  1.000                    0\n",
      "9867      0.000000       0.251288                  0.400                    0\n",
      "9868      0.050013       0.178947                  0.600                    0\n",
      "9869      0.037350       0.201344                  0.400                    0\n",
      "9870      0.160527       0.000000                  0.000                    1\n",
      "9871      0.161678       0.000000                  1.000                    0\n",
      "9872      0.039908       0.223516                  1.000                    0\n",
      "9873      0.178946       0.000000                  0.400                    0\n",
      "9874      0.182144       0.000000                  0.400                    0\n",
      "9875      0.058199       0.223516                  1.000                    0\n",
      "9876      0.061013       0.296529                  0.400                    0\n",
      "9877      0.149015       0.222844                  0.400                    0\n",
      "9878      0.254797       0.111982                  0.400                    0\n",
      "9879      0.074188       0.475924                  0.400                    0\n",
      "9880      0.408416       0.154311                  0.400                    0\n",
      "9881      0.468790       0.167973                  0.800                    0\n",
      "9882      0.520594       0.265174                  1.000                    0\n",
      "9883      0.698516       0.134378                  0.400                    0\n",
      "9884      0.635201       0.341769                  0.400                    0\n",
      "9885      1.000000       0.000000                  0.400                    0\n",
      "\n",
      "[9886 rows x 4 columns]\n",
      "\n",
      "Target feature:\n",
      " 0       0\n",
      "1       1\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      0\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      0\n",
      "18      0\n",
      "19      0\n",
      "20      1\n",
      "21      0\n",
      "22      0\n",
      "23      0\n",
      "24      0\n",
      "25      0\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "       ..\n",
      "9856    0\n",
      "9857    0\n",
      "9858    0\n",
      "9859    0\n",
      "9860    0\n",
      "9861    0\n",
      "9862    0\n",
      "9863    0\n",
      "9864    0\n",
      "9865    0\n",
      "9866    0\n",
      "9867    0\n",
      "9868    0\n",
      "9869    0\n",
      "9870    1\n",
      "9871    0\n",
      "9872    0\n",
      "9873    0\n",
      "9874    0\n",
      "9875    0\n",
      "9876    0\n",
      "9877    0\n",
      "9878    0\n",
      "9879    0\n",
      "9880    0\n",
      "9881    0\n",
      "9882    0\n",
      "9883    0\n",
      "9884    0\n",
      "9885    0\n",
      "Name: IsWinner, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare the descriptive features\n",
    "X = pd.concat([df[['ListingPrice','ShippingPrice','ShippingTime_maxHours','IsFulfilledByAmazon']]], axis=1)\n",
    "y = df.IsWinner \n",
    "print(\"Descriptive features:\\n\", X)\n",
    "print(\"\\nTarget feature:\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=1, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "#Train a decision tree classifier model\n",
    "#Instantiate estimator, fit with training set\n",
    "# Train a classification tree with max_depth=6 on all data\n",
    "dtc = DecisionTreeClassifier(max_depth=4, random_state=1)\n",
    "dtc.fit(X, y)\n",
    "print(dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ListingPrice</td>\n",
       "      <td>0.143740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ShippingPrice</td>\n",
       "      <td>0.067829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ShippingTime_maxHours</td>\n",
       "      <td>0.304042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IsFulfilledByAmazon</td>\n",
       "      <td>0.484389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature  importance\n",
       "0           ListingPrice    0.143740\n",
       "1          ShippingPrice    0.067829\n",
       "2  ShippingTime_maxHours    0.304042\n",
       "3    IsFulfilledByAmazon    0.484389"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the importance of each feature based on the trained decision tree classifier\n",
    "pd.DataFrame({'feature': X.columns, 'importance': dtc.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=dtc.predict(X)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.965405624115\n",
      "Confusion matrix: \n",
      " [[9326   13]\n",
      " [ 329  218]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      9339\n",
      "          1       0.94      0.40      0.56       547\n",
      "\n",
      "avg / total       0.96      0.97      0.96      9886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score(y, predictions))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y, predictions))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Using the better model (as per evaluation on training set), print the predicted target feature value for all the examples in the training set. Print the predicted class for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=dtc.predict(X)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted target feature :\n",
      "       ListingPrice  ShippingPrice  ShippingTime_maxHours  IsFulfilledByAmazon  \\\n",
      "0         0.000000       0.000000                  1.000                    1   \n",
      "1         0.235077       0.000000                  0.025                    0   \n",
      "2         0.113752       0.466311                  0.000                    0   \n",
      "3         0.092270       0.778519                  0.000                    0   \n",
      "4         0.272583       0.599733                  0.000                    0   \n",
      "5         0.399718       0.465644                  0.000                    0   \n",
      "6         0.540412       0.000000                  0.000                    0   \n",
      "7         0.459236       0.665777                  0.075                    0   \n",
      "8         0.558725       0.333556                  0.000                    0   \n",
      "9         0.480190       0.665777                  0.075                    0   \n",
      "10        0.688854       0.000000                  0.075                    0   \n",
      "11        0.524212       0.863909                  0.000                    0   \n",
      "12        0.802606       0.000000                  0.000                    0   \n",
      "13        0.752069       0.369580                  0.000                    0   \n",
      "14        0.655925       1.000000                  0.000                    0   \n",
      "15        0.814580       0.466978                  0.000                    0   \n",
      "16        0.914422       0.089393                  0.075                    0   \n",
      "17        0.990843       0.000000                  0.050                    0   \n",
      "18        1.000000       0.000000                  0.050                    0   \n",
      "19        0.015816       0.217796                  0.000                    0   \n",
      "20        0.053887       0.000000                  0.075                    0   \n",
      "21        0.058109       0.000000                  1.000                    1   \n",
      "22        0.025680       0.197780                  0.075                    0   \n",
      "23        0.029383       0.197780                  0.075                    0   \n",
      "24        0.000000       0.429845                  0.000                    0   \n",
      "25        0.040424       0.224138                  0.000                    0   \n",
      "26        0.069461       0.099088                  0.000                    0   \n",
      "27        0.080916       0.099088                  0.000                    0   \n",
      "28        0.099121       0.261395                  0.075                    0   \n",
      "29        0.121340       0.138724                  0.000                    0   \n",
      "...            ...            ...                    ...                  ...   \n",
      "9856      0.061013       0.296529                  0.400                    0   \n",
      "9857      0.149015       0.222844                  0.400                    0   \n",
      "9858      0.254797       0.111982                  0.400                    0   \n",
      "9859      0.074188       0.475924                  0.400                    0   \n",
      "9860      0.408416       0.154311                  0.400                    0   \n",
      "9861      0.468790       0.167973                  0.800                    0   \n",
      "9862      0.520594       0.265174                  1.000                    0   \n",
      "9863      0.698516       0.134378                  0.400                    0   \n",
      "9864      0.635201       0.341769                  0.400                    0   \n",
      "9865      1.000000       0.000000                  0.400                    0   \n",
      "9866      0.697109       1.000000                  1.000                    0   \n",
      "9867      0.000000       0.251288                  0.400                    0   \n",
      "9868      0.050013       0.178947                  0.600                    0   \n",
      "9869      0.037350       0.201344                  0.400                    0   \n",
      "9870      0.160527       0.000000                  0.000                    1   \n",
      "9871      0.161678       0.000000                  1.000                    0   \n",
      "9872      0.039908       0.223516                  1.000                    0   \n",
      "9873      0.178946       0.000000                  0.400                    0   \n",
      "9874      0.182144       0.000000                  0.400                    0   \n",
      "9875      0.058199       0.223516                  1.000                    0   \n",
      "9876      0.061013       0.296529                  0.400                    0   \n",
      "9877      0.149015       0.222844                  0.400                    0   \n",
      "9878      0.254797       0.111982                  0.400                    0   \n",
      "9879      0.074188       0.475924                  0.400                    0   \n",
      "9880      0.408416       0.154311                  0.400                    0   \n",
      "9881      0.468790       0.167973                  0.800                    0   \n",
      "9882      0.520594       0.265174                  1.000                    0   \n",
      "9883      0.698516       0.134378                  0.400                    0   \n",
      "9884      0.635201       0.341769                  0.400                    0   \n",
      "9885      1.000000       0.000000                  0.400                    0   \n",
      "\n",
      "      IsWinner  predicted  \n",
      "0            0          0  \n",
      "1            1          0  \n",
      "2            0          0  \n",
      "3            0          0  \n",
      "4            0          0  \n",
      "5            0          0  \n",
      "6            0          0  \n",
      "7            0          0  \n",
      "8            0          0  \n",
      "9            0          0  \n",
      "10           0          0  \n",
      "11           0          0  \n",
      "12           0          0  \n",
      "13           0          0  \n",
      "14           0          0  \n",
      "15           0          0  \n",
      "16           0          0  \n",
      "17           0          0  \n",
      "18           0          0  \n",
      "19           0          0  \n",
      "20           1          0  \n",
      "21           0          0  \n",
      "22           0          0  \n",
      "23           0          0  \n",
      "24           0          0  \n",
      "25           0          0  \n",
      "26           0          0  \n",
      "27           0          0  \n",
      "28           0          0  \n",
      "29           0          0  \n",
      "...        ...        ...  \n",
      "9856         0          0  \n",
      "9857         0          0  \n",
      "9858         0          0  \n",
      "9859         0          0  \n",
      "9860         0          0  \n",
      "9861         0          0  \n",
      "9862         0          0  \n",
      "9863         0          0  \n",
      "9864         0          0  \n",
      "9865         0          0  \n",
      "9866         0          0  \n",
      "9867         0          0  \n",
      "9868         0          0  \n",
      "9869         0          0  \n",
      "9870         1          1  \n",
      "9871         0          0  \n",
      "9872         0          0  \n",
      "9873         0          0  \n",
      "9874         0          0  \n",
      "9875         0          0  \n",
      "9876         0          0  \n",
      "9877         0          0  \n",
      "9878         0          0  \n",
      "9879         0          0  \n",
      "9880         0          0  \n",
      "9881         0          0  \n",
      "9882         0          0  \n",
      "9883         0          0  \n",
      "9884         0          0  \n",
      "9885         0          0  \n",
      "\n",
      "[9886 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "predicted = pd.DataFrame({'predicted': predictions})\n",
    "print(\"The predicted target feature :\\n\", pd.concat([X,y,predicted], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Evaluating Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Split the dataset into 70% training and remaining 30% test. Train all models from the previous exercises using the new training set and evaluate their quality on the new test set. Print classification evaluation metrics for all models on the test set (e.g., Accuracy, Confusion matrix, Precision, Recall, F1). Discuss how does evaluation on the test set compare to evaluation using the full data for training and also for testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive features:\n",
      "       ListingPrice  ShippingPrice  SellerFeedbackRating  \\\n",
      "0         0.000000       0.000000              0.000000   \n",
      "1         0.235077       0.000000              0.950000   \n",
      "2         0.113752       0.466311              0.980000   \n",
      "3         0.092270       0.778519              0.950000   \n",
      "4         0.272583       0.599733              0.940000   \n",
      "5         0.399718       0.465644              0.670000   \n",
      "6         0.540412       0.000000              1.000000   \n",
      "7         0.459236       0.665777              0.960000   \n",
      "8         0.558725       0.333556              0.910000   \n",
      "9         0.480190       0.665777              0.960000   \n",
      "10        0.688854       0.000000              0.960000   \n",
      "11        0.524212       0.863909              0.960000   \n",
      "12        0.802606       0.000000              0.940000   \n",
      "13        0.752069       0.369580              1.000000   \n",
      "14        0.655925       1.000000              0.940000   \n",
      "15        0.814580       0.466978              0.960000   \n",
      "16        0.914422       0.089393              1.000000   \n",
      "17        0.990843       0.000000              1.000000   \n",
      "18        1.000000       0.000000              1.000000   \n",
      "19        0.015816       0.217796              0.989899   \n",
      "20        0.053887       0.000000              0.969697   \n",
      "21        0.058109       0.000000              0.000000   \n",
      "22        0.025680       0.197780              0.969697   \n",
      "23        0.029383       0.197780              0.969697   \n",
      "24        0.000000       0.429845              0.939394   \n",
      "25        0.040424       0.224138              0.959596   \n",
      "26        0.069461       0.099088              0.919192   \n",
      "27        0.080916       0.099088              0.919192   \n",
      "28        0.099121       0.261395              0.888889   \n",
      "29        0.121340       0.138724              0.969697   \n",
      "...            ...            ...                   ...   \n",
      "9856      0.061013       0.296529              1.000000   \n",
      "9857      0.149015       0.222844              0.960000   \n",
      "9858      0.254797       0.111982              0.910000   \n",
      "9859      0.074188       0.475924              0.930000   \n",
      "9860      0.408416       0.154311              1.000000   \n",
      "9861      0.468790       0.167973              0.910000   \n",
      "9862      0.520594       0.265174              0.880000   \n",
      "9863      0.698516       0.134378              0.960000   \n",
      "9864      0.635201       0.341769              0.000000   \n",
      "9865      1.000000       0.000000              0.980000   \n",
      "9866      0.697109       1.000000              0.960000   \n",
      "9867      0.000000       0.251288              0.950000   \n",
      "9868      0.050013       0.178947              0.980000   \n",
      "9869      0.037350       0.201344              0.840000   \n",
      "9870      0.160527       0.000000              0.000000   \n",
      "9871      0.161678       0.000000              0.960000   \n",
      "9872      0.039908       0.223516              0.960000   \n",
      "9873      0.178946       0.000000              0.940000   \n",
      "9874      0.182144       0.000000              1.000000   \n",
      "9875      0.058199       0.223516              0.960000   \n",
      "9876      0.061013       0.296529              1.000000   \n",
      "9877      0.149015       0.222844              0.960000   \n",
      "9878      0.254797       0.111982              0.910000   \n",
      "9879      0.074188       0.475924              0.930000   \n",
      "9880      0.408416       0.154311              1.000000   \n",
      "9881      0.468790       0.167973              0.910000   \n",
      "9882      0.520594       0.265174              0.880000   \n",
      "9883      0.698516       0.134378              0.960000   \n",
      "9884      0.635201       0.341769              0.000000   \n",
      "9885      1.000000       0.000000              0.980000   \n",
      "\n",
      "      ShippingTime_maxHours  IsFulfilledByAmazon  IsFeaturedMerchant  \n",
      "0                     1.000                    1                   1  \n",
      "1                     0.025                    0                   1  \n",
      "2                     0.000                    0                   1  \n",
      "3                     0.000                    0                   1  \n",
      "4                     0.000                    0                   0  \n",
      "5                     0.000                    0                   0  \n",
      "6                     0.000                    0                   1  \n",
      "7                     0.075                    0                   1  \n",
      "8                     0.000                    0                   1  \n",
      "9                     0.075                    0                   1  \n",
      "10                    0.075                    0                   1  \n",
      "11                    0.000                    0                   1  \n",
      "12                    0.000                    0                   1  \n",
      "13                    0.000                    0                   1  \n",
      "14                    0.000                    0                   1  \n",
      "15                    0.000                    0                   1  \n",
      "16                    0.075                    0                   1  \n",
      "17                    0.050                    0                   1  \n",
      "18                    0.050                    0                   1  \n",
      "19                    0.000                    0                   0  \n",
      "20                    0.075                    0                   1  \n",
      "21                    1.000                    1                   1  \n",
      "22                    0.075                    0                   1  \n",
      "23                    0.075                    0                   1  \n",
      "24                    0.000                    0                   1  \n",
      "25                    0.000                    0                   1  \n",
      "26                    0.000                    0                   1  \n",
      "27                    0.000                    0                   1  \n",
      "28                    0.075                    0                   1  \n",
      "29                    0.000                    0                   1  \n",
      "...                     ...                  ...                 ...  \n",
      "9856                  0.400                    0                   1  \n",
      "9857                  0.400                    0                   1  \n",
      "9858                  0.400                    0                   1  \n",
      "9859                  0.400                    0                   1  \n",
      "9860                  0.400                    0                   1  \n",
      "9861                  0.800                    0                   1  \n",
      "9862                  1.000                    0                   1  \n",
      "9863                  0.400                    0                   1  \n",
      "9864                  0.400                    0                   1  \n",
      "9865                  0.400                    0                   1  \n",
      "9866                  1.000                    0                   1  \n",
      "9867                  0.400                    0                   1  \n",
      "9868                  0.600                    0                   0  \n",
      "9869                  0.400                    0                   0  \n",
      "9870                  0.000                    1                   1  \n",
      "9871                  1.000                    0                   1  \n",
      "9872                  1.000                    0                   1  \n",
      "9873                  0.400                    0                   1  \n",
      "9874                  0.400                    0                   1  \n",
      "9875                  1.000                    0                   1  \n",
      "9876                  0.400                    0                   1  \n",
      "9877                  0.400                    0                   1  \n",
      "9878                  0.400                    0                   1  \n",
      "9879                  0.400                    0                   1  \n",
      "9880                  0.400                    0                   1  \n",
      "9881                  0.800                    0                   1  \n",
      "9882                  1.000                    0                   1  \n",
      "9883                  0.400                    0                   1  \n",
      "9884                  0.400                    0                   1  \n",
      "9885                  0.400                    0                   1  \n",
      "\n",
      "[9886 rows x 6 columns]\n",
      "\n",
      "Target feature:\n",
      " 0       0\n",
      "1       1\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      0\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      0\n",
      "18      0\n",
      "19      0\n",
      "20      1\n",
      "21      0\n",
      "22      0\n",
      "23      0\n",
      "24      0\n",
      "25      0\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "       ..\n",
      "9856    0\n",
      "9857    0\n",
      "9858    0\n",
      "9859    0\n",
      "9860    0\n",
      "9861    0\n",
      "9862    0\n",
      "9863    0\n",
      "9864    0\n",
      "9865    0\n",
      "9866    0\n",
      "9867    0\n",
      "9868    0\n",
      "9869    0\n",
      "9870    1\n",
      "9871    0\n",
      "9872    0\n",
      "9873    0\n",
      "9874    0\n",
      "9875    0\n",
      "9876    0\n",
      "9877    0\n",
      "9878    0\n",
      "9879    0\n",
      "9880    0\n",
      "9881    0\n",
      "9882    0\n",
      "9883    0\n",
      "9884    0\n",
      "9885    0\n",
      "Name: IsWinner, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare the descriptive features\n",
    "X = pd.concat([df[['ListingPrice','ShippingPrice','SellerFeedbackRating','ShippingTime_maxHours','IsFulfilledByAmazon','IsFeaturedMerchant']]], axis=1)\n",
    "y = df.IsWinner \n",
    "print(\"Descriptive features:\\n\", X)\n",
    "print(\"\\nTarget feature:\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "       ListingPrice  ShippingPrice  SellerFeedbackRating  \\\n",
      "4137      0.983060       0.000000              0.910000   \n",
      "7211      0.551899       0.192827              0.910000   \n",
      "6697      0.559318       0.201694              0.928571   \n",
      "4580      0.564505       0.377000              0.827586   \n",
      "1144      0.381084       0.000000              1.000000   \n",
      "2597      1.000000       0.000000              1.000000   \n",
      "4715      0.590707       1.000000              0.970000   \n",
      "5418      0.722639       0.000000              0.960000   \n",
      "5344      0.000000       0.199894              0.950000   \n",
      "7703      0.173187       0.000000              0.666667   \n",
      "6189      0.503405       0.000000              0.000000   \n",
      "1853      0.381103       0.167039              0.960000   \n",
      "8603      0.185387       0.918505              0.875000   \n",
      "340       0.003458       0.374344              0.969697   \n",
      "771       0.398990       0.367182              0.979592   \n",
      "7505      0.757283       0.000000              0.940000   \n",
      "9827      0.781549       0.769461              0.950000   \n",
      "4747      0.712464       0.000000              0.960000   \n",
      "6484      0.224364       0.000000              0.960000   \n",
      "2399      0.316901       0.333556              0.910000   \n",
      "3053      0.012448       0.215761              0.896552   \n",
      "2493      0.731514       0.000000              0.950000   \n",
      "3885      0.924122       0.162496              0.437500   \n",
      "3713      0.000000       0.543403              0.930000   \n",
      "8252      0.170266       0.726003              0.000000   \n",
      "8273      1.000000       0.426319              0.510204   \n",
      "1928      0.530433       0.608933              1.000000   \n",
      "3693      0.155936       0.000000              0.910000   \n",
      "5488      0.236493       0.000000              1.000000   \n",
      "48        0.232333       0.408314              0.970000   \n",
      "...            ...            ...                   ...   \n",
      "9512      0.062124       0.000000              0.979592   \n",
      "8867      0.973939       0.140331              0.928571   \n",
      "2078      0.296806       1.000000              0.950000   \n",
      "8533      0.131302       0.107745              0.969388   \n",
      "4287      0.275667       0.451264              0.910000   \n",
      "2286      0.295508       0.229574              0.950000   \n",
      "8307      0.308340       1.000000              0.989796   \n",
      "4770      0.639473       0.276753              0.800000   \n",
      "2447      1.000000       0.000000              0.940000   \n",
      "2332      0.736183       0.000000              0.910000   \n",
      "2387      0.759675       0.413422              0.960000   \n",
      "4895      0.046466       0.454027              0.970000   \n",
      "8897      0.293561       0.000000              0.200000   \n",
      "2577      1.000000       0.000000              0.969388   \n",
      "8318      0.374106       0.000000              0.250000   \n",
      "7636      0.378082       0.083306              0.689655   \n",
      "5413      0.302284       0.000000              0.960000   \n",
      "8746      0.000000       0.503996              0.950000   \n",
      "1123      0.135209       0.000000              0.910000   \n",
      "8300      0.014341       0.668890              0.969388   \n",
      "5590      0.197837       1.000000              0.333333   \n",
      "7936      0.424923       0.900722              0.979592   \n",
      "6857      0.426493       0.000000              1.000000   \n",
      "1435      0.834250       0.000000              0.950000   \n",
      "8123      0.509418       0.160211              0.980000   \n",
      "1093      0.696334       0.000000              0.437500   \n",
      "4727      0.717146       0.000000              1.000000   \n",
      "2106      0.049391       0.000000              0.960000   \n",
      "5192      0.874433       0.186699              1.000000   \n",
      "1681      0.899974       0.000000              0.980000   \n",
      "\n",
      "      ShippingTime_maxHours  IsFulfilledByAmazon  IsFeaturedMerchant  IsWinner  \n",
      "4137               0.200000                    0                   1         0  \n",
      "7211               0.200000                    0                   1         0  \n",
      "6697               0.200000                    0                   1         0  \n",
      "4580               0.000000                    0                   1         0  \n",
      "1144               0.000000                    0                   1         0  \n",
      "2597               0.666667                    0                   1         0  \n",
      "4715               0.200000                    0                   1         0  \n",
      "5418               0.200000                    0                   0         0  \n",
      "5344               0.000000                    0                   1         1  \n",
      "7703               0.000000                    0                   1         0  \n",
      "6189               0.000000                    1                   1         1  \n",
      "1853               0.000000                    0                   1         0  \n",
      "8603               0.000000                    0                   1         0  \n",
      "340                1.000000                    0                   1         0  \n",
      "771                0.500000                    0                   1         0  \n",
      "7505               0.000000                    0                   1         0  \n",
      "9827               0.000000                    0                   0         0  \n",
      "4747               0.075000                    0                   1         0  \n",
      "6484               0.075000                    0                   1         0  \n",
      "2399               0.400000                    0                   1         0  \n",
      "3053               0.000000                    0                   1         0  \n",
      "2493               1.000000                    0                   1         0  \n",
      "3885               0.000000                    0                   1         0  \n",
      "3713               0.200000                    0                   1         0  \n",
      "8252               0.375000                    0                   1         0  \n",
      "8273               0.000000                    0                   1         0  \n",
      "1928               0.000000                    0                   1         0  \n",
      "3693               0.200000                    0                   1         0  \n",
      "5488               0.200000                    0                   1         0  \n",
      "48                 1.000000                    0                   1         0  \n",
      "...                     ...                  ...                 ...       ...  \n",
      "9512               0.075000                    0                   1         0  \n",
      "8867               0.400000                    0                   1         0  \n",
      "2078               0.400000                    0                   1         0  \n",
      "8533               0.500000                    0                   1         0  \n",
      "4287               0.200000                    0                   1         0  \n",
      "2286               0.500000                    0                   1         0  \n",
      "8307               0.200000                    0                   1         0  \n",
      "4770               0.375000                    0                   1         0  \n",
      "2447               0.325000                    0                   0         0  \n",
      "2332               0.400000                    0                   1         0  \n",
      "2387               0.500000                    0                   1         0  \n",
      "4895               1.000000                    0                   1         0  \n",
      "8897               0.000000                    0                   1         0  \n",
      "2577               0.200000                    0                   1         0  \n",
      "8318               0.000000                    0                   1         0  \n",
      "7636               0.000000                    0                   1         0  \n",
      "5413               0.500000                    0                   1         0  \n",
      "8746               0.200000                    0                   1         0  \n",
      "1123               0.200000                    0                   1         0  \n",
      "8300               0.000000                    0                   1         0  \n",
      "5590               0.375000                    0                   1         0  \n",
      "7936               0.500000                    0                   1         0  \n",
      "6857               1.000000                    0                   1         0  \n",
      "1435               0.600000                    0                   1         0  \n",
      "8123               0.200000                    0                   1         0  \n",
      "1093               0.000000                    0                   1         0  \n",
      "4727               0.050000                    0                   1         0  \n",
      "2106               0.000000                    0                   0         0  \n",
      "5192               0.200000                    0                   0         0  \n",
      "1681               0.000000                    0                   0         0  \n",
      "\n",
      "[6920 rows x 7 columns]\n",
      "\n",
      "Test data:\n",
      "       ListingPrice  ShippingPrice  SellerFeedbackRating  \\\n",
      "9813      0.000000       0.969311              0.970000   \n",
      "8661      0.241686       0.444804              0.970000   \n",
      "9783      0.585762       1.000000              0.562500   \n",
      "7573      0.130352       0.000000              0.840000   \n",
      "6587      0.731879       0.169181              0.960000   \n",
      "8589      0.978590       0.089542              0.910000   \n",
      "6854      0.166603       0.237530              0.470588   \n",
      "4438      0.678912       0.000000              0.583333   \n",
      "5812      0.818484       0.043630              0.910000   \n",
      "646       0.000000       0.583596              0.989796   \n",
      "3675      0.384924       0.000000              1.000000   \n",
      "5644      0.184768       0.000000              0.950000   \n",
      "6190      0.587062       0.000000              0.960000   \n",
      "7100      0.848008       0.140647              0.910000   \n",
      "3079      0.286298       0.304289              0.960000   \n",
      "619       0.425838       0.000000              0.989899   \n",
      "9177      0.053239       0.000000              0.000000   \n",
      "9588      0.051109       0.564426              0.948980   \n",
      "7735      0.511910       0.000000              1.000000   \n",
      "687       0.155184       0.000000              0.960000   \n",
      "9514      0.077753       0.000000              0.928571   \n",
      "5119      0.218255       0.617078              0.750000   \n",
      "4352      0.602835       0.249377              0.910000   \n",
      "7261      0.502901       0.493092              0.950000   \n",
      "7419      0.227016       0.000000              0.687500   \n",
      "3952      0.330417       0.000000              1.000000   \n",
      "2403      0.657570       0.333556              0.910000   \n",
      "783       0.031689       0.370258              1.000000   \n",
      "1076      0.713746       0.000000              0.959596   \n",
      "7147      0.844696       0.046529              0.910000   \n",
      "...            ...            ...                   ...   \n",
      "3575      0.274243       0.248932              0.980000   \n",
      "7305      0.059391       0.434669              0.666667   \n",
      "8837      0.341252       0.179211              0.910000   \n",
      "8231      0.106429       0.000000              0.919192   \n",
      "8626      0.515889       0.712114              0.950000   \n",
      "4736      1.000000       0.000000              1.000000   \n",
      "1824      0.470735       0.525891              1.000000   \n",
      "694       0.369224       0.348430              0.960000   \n",
      "7191      0.329032       0.000000              0.979592   \n",
      "5741      0.356061       0.333556              0.910000   \n",
      "1201      0.886582       0.227894              0.437500   \n",
      "5677      1.000000       0.345146              0.500000   \n",
      "3671      0.235735       0.000000              0.687500   \n",
      "3413      0.011111       0.000000              0.880000   \n",
      "4         0.272583       0.599733              0.940000   \n",
      "2465      0.827464       0.000000              0.437500   \n",
      "6690      0.000000       0.522388              0.989796   \n",
      "5505      0.278466       0.512354              0.980000   \n",
      "53        0.022051       0.323299              0.827586   \n",
      "1626      0.021658       0.167184              0.980000   \n",
      "7729      0.154598       0.435468              0.960000   \n",
      "7367      0.340284       0.233889              0.687500   \n",
      "8222      0.339184       0.252017              0.812500   \n",
      "234       0.350357       0.136406              0.500000   \n",
      "8241      0.573832       0.000000              0.969697   \n",
      "5797      0.065464       0.000000              0.940000   \n",
      "3209      0.793540       0.223717              0.960000   \n",
      "4555      0.458520       0.000000              1.000000   \n",
      "5778      0.249353       0.000000              0.950000   \n",
      "3112      0.000000       1.000000              0.000000   \n",
      "\n",
      "      ShippingTime_maxHours  IsFulfilledByAmazon  IsFeaturedMerchant  IsWinner  \n",
      "9813               0.000000                    0                   0         0  \n",
      "8661               1.000000                    0                   1         0  \n",
      "9783               0.000000                    0                   1         0  \n",
      "7573               0.000000                    0                   0         0  \n",
      "6587               0.075000                    0                   1         0  \n",
      "8589               0.000000                    0                   1         0  \n",
      "6854               0.000000                    0                   1         0  \n",
      "4438               0.000000                    0                   1         0  \n",
      "5812               0.200000                    0                   1         0  \n",
      "646                0.400000                    0                   1         0  \n",
      "3675               0.000000                    0                   1         0  \n",
      "5644               0.300000                    0                   1         0  \n",
      "6190               1.000000                    0                   1         0  \n",
      "7100               0.400000                    0                   1         0  \n",
      "3079               0.000000                    0                   1         0  \n",
      "619                0.000000                    0                   1         0  \n",
      "9177               0.000000                    1                   1         0  \n",
      "9588               0.000000                    0                   1         0  \n",
      "7735               0.050000                    0                   1         0  \n",
      "687                0.000000                    0                   1         0  \n",
      "9514               0.000000                    0                   1         0  \n",
      "5119               1.000000                    0                   1         0  \n",
      "4352               0.400000                    0                   1         0  \n",
      "7261               0.000000                    0                   1         0  \n",
      "7419               0.333333                    0                   1         0  \n",
      "3952               0.400000                    0                   1         0  \n",
      "2403               0.400000                    0                   1         0  \n",
      "783                0.200000                    0                   0         0  \n",
      "1076               0.000000                    0                   1         0  \n",
      "7147               0.000000                    0                   1         0  \n",
      "...                     ...                  ...                 ...       ...  \n",
      "3575               0.200000                    0                   1         0  \n",
      "7305               0.375000                    0                   1         0  \n",
      "8837               0.400000                    0                   1         0  \n",
      "8231               0.000000                    0                   1         1  \n",
      "8626               0.400000                    0                   1         0  \n",
      "4736               0.025000                    0                   1         0  \n",
      "1824               0.400000                    0                   1         0  \n",
      "694                0.000000                    0                   1         0  \n",
      "7191               0.000000                    0                   0         0  \n",
      "5741               0.400000                    0                   0         0  \n",
      "1201               0.000000                    0                   1         0  \n",
      "5677               0.133333                    0                   1         0  \n",
      "3671               0.333333                    0                   1         0  \n",
      "3413               1.000000                    0                   1         1  \n",
      "4                  0.000000                    0                   0         0  \n",
      "2465               0.000000                    0                   1         0  \n",
      "6690               0.200000                    0                   1         0  \n",
      "5505               0.600000                    0                   0         0  \n",
      "53                 0.000000                    0                   1         0  \n",
      "1626               0.125000                    0                   0         0  \n",
      "7729               0.000000                    0                   1         0  \n",
      "7367               1.000000                    0                   1         0  \n",
      "8222               0.000000                    0                   1         0  \n",
      "234                0.153846                    0                   0         0  \n",
      "8241               0.000000                    0                   0         0  \n",
      "5797               0.200000                    0                   0         0  \n",
      "3209               0.375000                    0                   1         0  \n",
      "4555               0.000000                    0                   1         0  \n",
      "5778               0.025000                    0                   1         0  \n",
      "3112               0.000000                    0                   1         0  \n",
      "\n",
      "[2966 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "# Take a third (random) data samples as test data, rest as training data\n",
    "# Note that this training set if very small and the model will not be very reliable due to this sample size problem.\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print(\"Training data:\\n\", pd.concat([X_train, y_train], axis=1))\n",
    "print(\"\\nTest data:\\n\", pd.concat([X_test, y_test], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train the model with linear regression \n",
    "df_linear_train=pd.concat([X_train, y_train], axis=1)\n",
    "lintrain= sm.ols(formula=\"IsWinner ~  ListingPrice+ShippingPrice+ShippingTime_maxHours+IsFulfilledByAmazon+IsFeaturedMerchant\", data=df_linear_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept                0.077512\n",
      "ListingPrice            -0.127902\n",
      "ShippingPrice           -0.047058\n",
      "ShippingTime_maxHours   -0.084615\n",
      "IsFulfilledByAmazon      0.520944\n",
      "IsFeaturedMerchant       0.053341\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(lintrain.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9813</th>\n",
       "      <td>0.031898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8661</th>\n",
       "      <td>-0.005606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9783</th>\n",
       "      <td>0.008875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7573</th>\n",
       "      <td>0.060839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6587</th>\n",
       "      <td>0.022936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8589</th>\n",
       "      <td>0.001475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6854</th>\n",
       "      <td>0.098366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4438</th>\n",
       "      <td>0.044018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5812</th>\n",
       "      <td>0.007191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>0.069544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3675</th>\n",
       "      <td>0.081620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5644</th>\n",
       "      <td>0.081836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6190</th>\n",
       "      <td>-0.028849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7100</th>\n",
       "      <td>-0.018074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>0.079915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>0.076387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9177</th>\n",
       "      <td>0.644987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9588</th>\n",
       "      <td>0.097755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7735</th>\n",
       "      <td>0.061147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>0.111004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9514</th>\n",
       "      <td>0.120908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5119</th>\n",
       "      <td>-0.010716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4352</th>\n",
       "      <td>0.008168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7261</th>\n",
       "      <td>0.043327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7419</th>\n",
       "      <td>0.073612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3952</th>\n",
       "      <td>0.054746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2403</th>\n",
       "      <td>-0.002794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>0.039112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>0.039563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7147</th>\n",
       "      <td>0.020625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3575</th>\n",
       "      <td>0.067139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7305</th>\n",
       "      <td>0.071071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8837</th>\n",
       "      <td>0.044926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8231</th>\n",
       "      <td>0.117240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8626</th>\n",
       "      <td>-0.002487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4736</th>\n",
       "      <td>0.000835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>0.012051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>0.067232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7191</th>\n",
       "      <td>0.035428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>-0.017571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>0.006733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5677</th>\n",
       "      <td>-0.024573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3671</th>\n",
       "      <td>0.072496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3413</th>\n",
       "      <td>0.044817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>0.025018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6690</th>\n",
       "      <td>0.089347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5505</th>\n",
       "      <td>-0.032984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.112818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>0.056297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7729</th>\n",
       "      <td>0.090587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7367</th>\n",
       "      <td>-0.008292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8222</th>\n",
       "      <td>0.075611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0.013264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8241</th>\n",
       "      <td>0.004117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5797</th>\n",
       "      <td>0.052216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3209</th>\n",
       "      <td>-0.012901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>0.072207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5778</th>\n",
       "      <td>0.096844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112</th>\n",
       "      <td>0.083795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2966 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      predicted\n",
       "9813   0.031898\n",
       "8661  -0.005606\n",
       "9783   0.008875\n",
       "7573   0.060839\n",
       "6587   0.022936\n",
       "8589   0.001475\n",
       "6854   0.098366\n",
       "4438   0.044018\n",
       "5812   0.007191\n",
       "646    0.069544\n",
       "3675   0.081620\n",
       "5644   0.081836\n",
       "6190  -0.028849\n",
       "7100  -0.018074\n",
       "3079   0.079915\n",
       "619    0.076387\n",
       "9177   0.644987\n",
       "9588   0.097755\n",
       "7735   0.061147\n",
       "687    0.111004\n",
       "9514   0.120908\n",
       "5119  -0.010716\n",
       "4352   0.008168\n",
       "7261   0.043327\n",
       "7419   0.073612\n",
       "3952   0.054746\n",
       "2403  -0.002794\n",
       "783    0.039112\n",
       "1076   0.039563\n",
       "7147   0.020625\n",
       "...         ...\n",
       "3575   0.067139\n",
       "7305   0.071071\n",
       "8837   0.044926\n",
       "8231   0.117240\n",
       "8626  -0.002487\n",
       "4736   0.000835\n",
       "1824   0.012051\n",
       "694    0.067232\n",
       "7191   0.035428\n",
       "5741  -0.017571\n",
       "1201   0.006733\n",
       "5677  -0.024573\n",
       "3671   0.072496\n",
       "3413   0.044817\n",
       "4      0.014426\n",
       "2465   0.025018\n",
       "6690   0.089347\n",
       "5505  -0.032984\n",
       "53     0.112818\n",
       "1626   0.056297\n",
       "7729   0.090587\n",
       "7367  -0.008292\n",
       "8222   0.075611\n",
       "234    0.013264\n",
       "8241   0.004117\n",
       "5797   0.052216\n",
       "3209  -0.012901\n",
       "4555   0.072207\n",
       "5778   0.096844\n",
       "3112   0.083795\n",
       "\n",
       "[2966 rows x 1 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing=lintrain.predict(X_test)\n",
    "predicted = pd.DataFrame({'predicted': df_testing})\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ListingPrice</th>\n",
       "      <th>ShippingPrice</th>\n",
       "      <th>SellerFeedbackRating</th>\n",
       "      <th>ShippingTime_maxHours</th>\n",
       "      <th>IsFulfilledByAmazon</th>\n",
       "      <th>IsFeaturedMerchant</th>\n",
       "      <th>IsWinner</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9813</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8661</th>\n",
       "      <td>0.241686</td>\n",
       "      <td>0.444804</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.005606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9783</th>\n",
       "      <td>0.585762</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7573</th>\n",
       "      <td>0.130352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6587</th>\n",
       "      <td>0.731879</td>\n",
       "      <td>0.169181</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8589</th>\n",
       "      <td>0.978590</td>\n",
       "      <td>0.089542</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6854</th>\n",
       "      <td>0.166603</td>\n",
       "      <td>0.237530</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.098366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4438</th>\n",
       "      <td>0.678912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5812</th>\n",
       "      <td>0.818484</td>\n",
       "      <td>0.043630</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.583596</td>\n",
       "      <td>0.989796</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.069544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3675</th>\n",
       "      <td>0.384924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.081620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5644</th>\n",
       "      <td>0.184768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.081836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6190</th>\n",
       "      <td>0.587062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.028849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7100</th>\n",
       "      <td>0.848008</td>\n",
       "      <td>0.140647</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.018074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>0.286298</td>\n",
       "      <td>0.304289</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.079915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>0.425838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9177</th>\n",
       "      <td>0.053239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.644987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9588</th>\n",
       "      <td>0.051109</td>\n",
       "      <td>0.564426</td>\n",
       "      <td>0.948980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.097755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7735</th>\n",
       "      <td>0.511910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>0.155184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9514</th>\n",
       "      <td>0.077753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.120908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5119</th>\n",
       "      <td>0.218255</td>\n",
       "      <td>0.617078</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.010716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4352</th>\n",
       "      <td>0.602835</td>\n",
       "      <td>0.249377</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7261</th>\n",
       "      <td>0.502901</td>\n",
       "      <td>0.493092</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7419</th>\n",
       "      <td>0.227016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.073612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3952</th>\n",
       "      <td>0.330417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2403</th>\n",
       "      <td>0.657570</td>\n",
       "      <td>0.333556</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.002794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>0.031689</td>\n",
       "      <td>0.370258</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>0.713746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7147</th>\n",
       "      <td>0.844696</td>\n",
       "      <td>0.046529</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3575</th>\n",
       "      <td>0.274243</td>\n",
       "      <td>0.248932</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.067139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7305</th>\n",
       "      <td>0.059391</td>\n",
       "      <td>0.434669</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8837</th>\n",
       "      <td>0.341252</td>\n",
       "      <td>0.179211</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8231</th>\n",
       "      <td>0.106429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.117240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8626</th>\n",
       "      <td>0.515889</td>\n",
       "      <td>0.712114</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.002487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4736</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>0.470735</td>\n",
       "      <td>0.525891</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>0.369224</td>\n",
       "      <td>0.348430</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.067232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7191</th>\n",
       "      <td>0.329032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>0.356061</td>\n",
       "      <td>0.333556</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>0.886582</td>\n",
       "      <td>0.227894</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5677</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.345146</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.024573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3671</th>\n",
       "      <td>0.235735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3413</th>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.044817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.272583</td>\n",
       "      <td>0.599733</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>0.827464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.025018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6690</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>0.989796</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5505</th>\n",
       "      <td>0.278466</td>\n",
       "      <td>0.512354</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.032984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.022051</td>\n",
       "      <td>0.323299</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>0.021658</td>\n",
       "      <td>0.167184</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7729</th>\n",
       "      <td>0.154598</td>\n",
       "      <td>0.435468</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.090587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7367</th>\n",
       "      <td>0.340284</td>\n",
       "      <td>0.233889</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.008292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8222</th>\n",
       "      <td>0.339184</td>\n",
       "      <td>0.252017</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.075611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0.350357</td>\n",
       "      <td>0.136406</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8241</th>\n",
       "      <td>0.573832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5797</th>\n",
       "      <td>0.065464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3209</th>\n",
       "      <td>0.793540</td>\n",
       "      <td>0.223717</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.012901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>0.458520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5778</th>\n",
       "      <td>0.249353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.096844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2966 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ListingPrice  ShippingPrice  SellerFeedbackRating  \\\n",
       "9813      0.000000       0.969311              0.970000   \n",
       "8661      0.241686       0.444804              0.970000   \n",
       "9783      0.585762       1.000000              0.562500   \n",
       "7573      0.130352       0.000000              0.840000   \n",
       "6587      0.731879       0.169181              0.960000   \n",
       "8589      0.978590       0.089542              0.910000   \n",
       "6854      0.166603       0.237530              0.470588   \n",
       "4438      0.678912       0.000000              0.583333   \n",
       "5812      0.818484       0.043630              0.910000   \n",
       "646       0.000000       0.583596              0.989796   \n",
       "3675      0.384924       0.000000              1.000000   \n",
       "5644      0.184768       0.000000              0.950000   \n",
       "6190      0.587062       0.000000              0.960000   \n",
       "7100      0.848008       0.140647              0.910000   \n",
       "3079      0.286298       0.304289              0.960000   \n",
       "619       0.425838       0.000000              0.989899   \n",
       "9177      0.053239       0.000000              0.000000   \n",
       "9588      0.051109       0.564426              0.948980   \n",
       "7735      0.511910       0.000000              1.000000   \n",
       "687       0.155184       0.000000              0.960000   \n",
       "9514      0.077753       0.000000              0.928571   \n",
       "5119      0.218255       0.617078              0.750000   \n",
       "4352      0.602835       0.249377              0.910000   \n",
       "7261      0.502901       0.493092              0.950000   \n",
       "7419      0.227016       0.000000              0.687500   \n",
       "3952      0.330417       0.000000              1.000000   \n",
       "2403      0.657570       0.333556              0.910000   \n",
       "783       0.031689       0.370258              1.000000   \n",
       "1076      0.713746       0.000000              0.959596   \n",
       "7147      0.844696       0.046529              0.910000   \n",
       "...            ...            ...                   ...   \n",
       "3575      0.274243       0.248932              0.980000   \n",
       "7305      0.059391       0.434669              0.666667   \n",
       "8837      0.341252       0.179211              0.910000   \n",
       "8231      0.106429       0.000000              0.919192   \n",
       "8626      0.515889       0.712114              0.950000   \n",
       "4736      1.000000       0.000000              1.000000   \n",
       "1824      0.470735       0.525891              1.000000   \n",
       "694       0.369224       0.348430              0.960000   \n",
       "7191      0.329032       0.000000              0.979592   \n",
       "5741      0.356061       0.333556              0.910000   \n",
       "1201      0.886582       0.227894              0.437500   \n",
       "5677      1.000000       0.345146              0.500000   \n",
       "3671      0.235735       0.000000              0.687500   \n",
       "3413      0.011111       0.000000              0.880000   \n",
       "4         0.272583       0.599733              0.940000   \n",
       "2465      0.827464       0.000000              0.437500   \n",
       "6690      0.000000       0.522388              0.989796   \n",
       "5505      0.278466       0.512354              0.980000   \n",
       "53        0.022051       0.323299              0.827586   \n",
       "1626      0.021658       0.167184              0.980000   \n",
       "7729      0.154598       0.435468              0.960000   \n",
       "7367      0.340284       0.233889              0.687500   \n",
       "8222      0.339184       0.252017              0.812500   \n",
       "234       0.350357       0.136406              0.500000   \n",
       "8241      0.573832       0.000000              0.969697   \n",
       "5797      0.065464       0.000000              0.940000   \n",
       "3209      0.793540       0.223717              0.960000   \n",
       "4555      0.458520       0.000000              1.000000   \n",
       "5778      0.249353       0.000000              0.950000   \n",
       "3112      0.000000       1.000000              0.000000   \n",
       "\n",
       "      ShippingTime_maxHours  IsFulfilledByAmazon  IsFeaturedMerchant  \\\n",
       "9813               0.000000                    0                   0   \n",
       "8661               1.000000                    0                   1   \n",
       "9783               0.000000                    0                   1   \n",
       "7573               0.000000                    0                   0   \n",
       "6587               0.075000                    0                   1   \n",
       "8589               0.000000                    0                   1   \n",
       "6854               0.000000                    0                   1   \n",
       "4438               0.000000                    0                   1   \n",
       "5812               0.200000                    0                   1   \n",
       "646                0.400000                    0                   1   \n",
       "3675               0.000000                    0                   1   \n",
       "5644               0.300000                    0                   1   \n",
       "6190               1.000000                    0                   1   \n",
       "7100               0.400000                    0                   1   \n",
       "3079               0.000000                    0                   1   \n",
       "619                0.000000                    0                   1   \n",
       "9177               0.000000                    1                   1   \n",
       "9588               0.000000                    0                   1   \n",
       "7735               0.050000                    0                   1   \n",
       "687                0.000000                    0                   1   \n",
       "9514               0.000000                    0                   1   \n",
       "5119               1.000000                    0                   1   \n",
       "4352               0.400000                    0                   1   \n",
       "7261               0.000000                    0                   1   \n",
       "7419               0.333333                    0                   1   \n",
       "3952               0.400000                    0                   1   \n",
       "2403               0.400000                    0                   1   \n",
       "783                0.200000                    0                   0   \n",
       "1076               0.000000                    0                   1   \n",
       "7147               0.000000                    0                   1   \n",
       "...                     ...                  ...                 ...   \n",
       "3575               0.200000                    0                   1   \n",
       "7305               0.375000                    0                   1   \n",
       "8837               0.400000                    0                   1   \n",
       "8231               0.000000                    0                   1   \n",
       "8626               0.400000                    0                   1   \n",
       "4736               0.025000                    0                   1   \n",
       "1824               0.400000                    0                   1   \n",
       "694                0.000000                    0                   1   \n",
       "7191               0.000000                    0                   0   \n",
       "5741               0.400000                    0                   0   \n",
       "1201               0.000000                    0                   1   \n",
       "5677               0.133333                    0                   1   \n",
       "3671               0.333333                    0                   1   \n",
       "3413               1.000000                    0                   1   \n",
       "4                  0.000000                    0                   0   \n",
       "2465               0.000000                    0                   1   \n",
       "6690               0.200000                    0                   1   \n",
       "5505               0.600000                    0                   0   \n",
       "53                 0.000000                    0                   1   \n",
       "1626               0.125000                    0                   0   \n",
       "7729               0.000000                    0                   1   \n",
       "7367               1.000000                    0                   1   \n",
       "8222               0.000000                    0                   1   \n",
       "234                0.153846                    0                   0   \n",
       "8241               0.000000                    0                   0   \n",
       "5797               0.200000                    0                   0   \n",
       "3209               0.375000                    0                   1   \n",
       "4555               0.000000                    0                   1   \n",
       "5778               0.025000                    0                   1   \n",
       "3112               0.000000                    0                   1   \n",
       "\n",
       "      IsWinner  predicted  \n",
       "9813         0   0.031898  \n",
       "8661         0  -0.005606  \n",
       "9783         0   0.008875  \n",
       "7573         0   0.060839  \n",
       "6587         0   0.022936  \n",
       "8589         0   0.001475  \n",
       "6854         0   0.098366  \n",
       "4438         0   0.044018  \n",
       "5812         0   0.007191  \n",
       "646          0   0.069544  \n",
       "3675         0   0.081620  \n",
       "5644         0   0.081836  \n",
       "6190         0  -0.028849  \n",
       "7100         0  -0.018074  \n",
       "3079         0   0.079915  \n",
       "619          0   0.076387  \n",
       "9177         0   0.644987  \n",
       "9588         0   0.097755  \n",
       "7735         0   0.061147  \n",
       "687          0   0.111004  \n",
       "9514         0   0.120908  \n",
       "5119         0  -0.010716  \n",
       "4352         0   0.008168  \n",
       "7261         0   0.043327  \n",
       "7419         0   0.073612  \n",
       "3952         0   0.054746  \n",
       "2403         0  -0.002794  \n",
       "783          0   0.039112  \n",
       "1076         0   0.039563  \n",
       "7147         0   0.020625  \n",
       "...        ...        ...  \n",
       "3575         0   0.067139  \n",
       "7305         0   0.071071  \n",
       "8837         0   0.044926  \n",
       "8231         1   0.117240  \n",
       "8626         0  -0.002487  \n",
       "4736         0   0.000835  \n",
       "1824         0   0.012051  \n",
       "694          0   0.067232  \n",
       "7191         0   0.035428  \n",
       "5741         0  -0.017571  \n",
       "1201         0   0.006733  \n",
       "5677         0  -0.024573  \n",
       "3671         0   0.072496  \n",
       "3413         1   0.044817  \n",
       "4            0   0.014426  \n",
       "2465         0   0.025018  \n",
       "6690         0   0.089347  \n",
       "5505         0  -0.032984  \n",
       "53           0   0.112818  \n",
       "1626         0   0.056297  \n",
       "7729         0   0.090587  \n",
       "7367         0  -0.008292  \n",
       "8222         0   0.075611  \n",
       "234          0   0.013264  \n",
       "8241         0   0.004117  \n",
       "5797         0   0.052216  \n",
       "3209         0  -0.012901  \n",
       "4555         0   0.072207  \n",
       "5778         0   0.096844  \n",
       "3112         0   0.083795  \n",
       "\n",
       "[2966 rows x 8 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results=pd.concat([X_test, y_test,predicted], axis=1)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "\n",
    "for i in lintrain.predict(X_test):\n",
    "    if i>0.5:\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "        \n",
    "predictions\n",
    "\n",
    "predicted = pd.DataFrame({'predicted': predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.949426837492\n",
      "Confusion matrix: \n",
      " [[2754   43]\n",
      " [ 107   62]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.98      0.97      2797\n",
      "          1       0.59      0.37      0.45       169\n",
      "\n",
      "avg / total       0.94      0.95      0.94      2966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score(y_test,predicted))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, predicted))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive features:\n",
      "       ListingPrice  ShippingPrice  ShippingTime_maxHours  IsFulfilledByAmazon  \\\n",
      "0         0.000000       0.000000                  1.000                    1   \n",
      "1         0.235077       0.000000                  0.025                    0   \n",
      "2         0.113752       0.466311                  0.000                    0   \n",
      "3         0.092270       0.778519                  0.000                    0   \n",
      "4         0.272583       0.599733                  0.000                    0   \n",
      "5         0.399718       0.465644                  0.000                    0   \n",
      "6         0.540412       0.000000                  0.000                    0   \n",
      "7         0.459236       0.665777                  0.075                    0   \n",
      "8         0.558725       0.333556                  0.000                    0   \n",
      "9         0.480190       0.665777                  0.075                    0   \n",
      "10        0.688854       0.000000                  0.075                    0   \n",
      "11        0.524212       0.863909                  0.000                    0   \n",
      "12        0.802606       0.000000                  0.000                    0   \n",
      "13        0.752069       0.369580                  0.000                    0   \n",
      "14        0.655925       1.000000                  0.000                    0   \n",
      "15        0.814580       0.466978                  0.000                    0   \n",
      "16        0.914422       0.089393                  0.075                    0   \n",
      "17        0.990843       0.000000                  0.050                    0   \n",
      "18        1.000000       0.000000                  0.050                    0   \n",
      "19        0.015816       0.217796                  0.000                    0   \n",
      "20        0.053887       0.000000                  0.075                    0   \n",
      "21        0.058109       0.000000                  1.000                    1   \n",
      "22        0.025680       0.197780                  0.075                    0   \n",
      "23        0.029383       0.197780                  0.075                    0   \n",
      "24        0.000000       0.429845                  0.000                    0   \n",
      "25        0.040424       0.224138                  0.000                    0   \n",
      "26        0.069461       0.099088                  0.000                    0   \n",
      "27        0.080916       0.099088                  0.000                    0   \n",
      "28        0.099121       0.261395                  0.075                    0   \n",
      "29        0.121340       0.138724                  0.000                    0   \n",
      "...            ...            ...                    ...                  ...   \n",
      "9856      0.061013       0.296529                  0.400                    0   \n",
      "9857      0.149015       0.222844                  0.400                    0   \n",
      "9858      0.254797       0.111982                  0.400                    0   \n",
      "9859      0.074188       0.475924                  0.400                    0   \n",
      "9860      0.408416       0.154311                  0.400                    0   \n",
      "9861      0.468790       0.167973                  0.800                    0   \n",
      "9862      0.520594       0.265174                  1.000                    0   \n",
      "9863      0.698516       0.134378                  0.400                    0   \n",
      "9864      0.635201       0.341769                  0.400                    0   \n",
      "9865      1.000000       0.000000                  0.400                    0   \n",
      "9866      0.697109       1.000000                  1.000                    0   \n",
      "9867      0.000000       0.251288                  0.400                    0   \n",
      "9868      0.050013       0.178947                  0.600                    0   \n",
      "9869      0.037350       0.201344                  0.400                    0   \n",
      "9870      0.160527       0.000000                  0.000                    1   \n",
      "9871      0.161678       0.000000                  1.000                    0   \n",
      "9872      0.039908       0.223516                  1.000                    0   \n",
      "9873      0.178946       0.000000                  0.400                    0   \n",
      "9874      0.182144       0.000000                  0.400                    0   \n",
      "9875      0.058199       0.223516                  1.000                    0   \n",
      "9876      0.061013       0.296529                  0.400                    0   \n",
      "9877      0.149015       0.222844                  0.400                    0   \n",
      "9878      0.254797       0.111982                  0.400                    0   \n",
      "9879      0.074188       0.475924                  0.400                    0   \n",
      "9880      0.408416       0.154311                  0.400                    0   \n",
      "9881      0.468790       0.167973                  0.800                    0   \n",
      "9882      0.520594       0.265174                  1.000                    0   \n",
      "9883      0.698516       0.134378                  0.400                    0   \n",
      "9884      0.635201       0.341769                  0.400                    0   \n",
      "9885      1.000000       0.000000                  0.400                    0   \n",
      "\n",
      "      IsFeaturedMerchant  \n",
      "0                      1  \n",
      "1                      1  \n",
      "2                      1  \n",
      "3                      1  \n",
      "4                      0  \n",
      "5                      0  \n",
      "6                      1  \n",
      "7                      1  \n",
      "8                      1  \n",
      "9                      1  \n",
      "10                     1  \n",
      "11                     1  \n",
      "12                     1  \n",
      "13                     1  \n",
      "14                     1  \n",
      "15                     1  \n",
      "16                     1  \n",
      "17                     1  \n",
      "18                     1  \n",
      "19                     0  \n",
      "20                     1  \n",
      "21                     1  \n",
      "22                     1  \n",
      "23                     1  \n",
      "24                     1  \n",
      "25                     1  \n",
      "26                     1  \n",
      "27                     1  \n",
      "28                     1  \n",
      "29                     1  \n",
      "...                  ...  \n",
      "9856                   1  \n",
      "9857                   1  \n",
      "9858                   1  \n",
      "9859                   1  \n",
      "9860                   1  \n",
      "9861                   1  \n",
      "9862                   1  \n",
      "9863                   1  \n",
      "9864                   1  \n",
      "9865                   1  \n",
      "9866                   1  \n",
      "9867                   1  \n",
      "9868                   0  \n",
      "9869                   0  \n",
      "9870                   1  \n",
      "9871                   1  \n",
      "9872                   1  \n",
      "9873                   1  \n",
      "9874                   1  \n",
      "9875                   1  \n",
      "9876                   1  \n",
      "9877                   1  \n",
      "9878                   1  \n",
      "9879                   1  \n",
      "9880                   1  \n",
      "9881                   1  \n",
      "9882                   1  \n",
      "9883                   1  \n",
      "9884                   1  \n",
      "9885                   1  \n",
      "\n",
      "[9886 rows x 5 columns]\n",
      "\n",
      "Target feature:\n",
      " 0       0\n",
      "1       1\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      0\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      0\n",
      "18      0\n",
      "19      0\n",
      "20      1\n",
      "21      0\n",
      "22      0\n",
      "23      0\n",
      "24      0\n",
      "25      0\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "       ..\n",
      "9856    0\n",
      "9857    0\n",
      "9858    0\n",
      "9859    0\n",
      "9860    0\n",
      "9861    0\n",
      "9862    0\n",
      "9863    0\n",
      "9864    0\n",
      "9865    0\n",
      "9866    0\n",
      "9867    0\n",
      "9868    0\n",
      "9869    0\n",
      "9870    1\n",
      "9871    0\n",
      "9872    0\n",
      "9873    0\n",
      "9874    0\n",
      "9875    0\n",
      "9876    0\n",
      "9877    0\n",
      "9878    0\n",
      "9879    0\n",
      "9880    0\n",
      "9881    0\n",
      "9882    0\n",
      "9883    0\n",
      "9884    0\n",
      "9885    0\n",
      "Name: IsWinner, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare the descriptive features\n",
    "X = pd.concat([df[['ListingPrice','ShippingPrice','ShippingTime_maxHours','IsFulfilledByAmazon','IsFeaturedMerchant']]], axis=1)\n",
    "y = df.IsWinner \n",
    "print(\"Descriptive features:\\n\", X)\n",
    "print(\"\\nTarget feature:\\n\", y)\n",
    "# Split the data into train and test sets\n",
    "# Take a third (random) data samples as test data, rest as training data\n",
    "# Note that this training set if very small and the model will not be very reliable due to this sample size problem.\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "       ListingPrice  ShippingPrice  ShippingTime_maxHours  IsFulfilledByAmazon  \\\n",
      "7315      0.591262       0.000000               0.000000                    0   \n",
      "7171      0.108595       0.000000               0.100000                    0   \n",
      "9257      0.186457       0.091637               0.200000                    0   \n",
      "2821      0.430522       0.164474               0.000000                    0   \n",
      "2102      0.814669       0.578386               0.666667                    0   \n",
      "837       0.207623       0.219100               0.000000                    0   \n",
      "6241      1.000000       0.000000               0.400000                    0   \n",
      "1057      0.558725       0.333556               0.000000                    0   \n",
      "785       0.081352       0.184626               0.000000                    0   \n",
      "7739      0.591929       0.000000               0.075000                    0   \n",
      "7108      0.759858       0.029760               0.375000                    0   \n",
      "5618      0.545134       0.600854               0.000000                    0   \n",
      "6425      0.100429       0.000000               1.000000                    1   \n",
      "4840      0.387147       0.749440               1.000000                    0   \n",
      "6968      0.002557       0.215307               0.000000                    0   \n",
      "6120      0.282615       0.000000               0.200000                    0   \n",
      "209       1.000000       0.062416               0.075000                    0   \n",
      "2389      0.695341       0.850041               0.500000                    0   \n",
      "4597      0.578008       0.000000               0.400000                    0   \n",
      "6803      0.114615       0.267371               0.200000                    0   \n",
      "1427      0.446956       0.000000               0.400000                    0   \n",
      "9316      0.040036       0.251288               0.400000                    0   \n",
      "5243      0.324153       0.134264               0.200000                    0   \n",
      "3098      0.232512       0.245263               0.125000                    0   \n",
      "5810      0.767918       0.043630               0.200000                    0   \n",
      "7972      0.510459       0.845807               0.000000                    0   \n",
      "6065      0.374106       0.000000               0.000000                    0   \n",
      "5097      0.962083       0.092872               0.075000                    0   \n",
      "5402      0.035468       0.000000               0.500000                    0   \n",
      "747       0.150973       0.382039               0.133333                    0   \n",
      "...            ...            ...                    ...                  ...   \n",
      "6952      0.646201       0.161000               0.333333                    0   \n",
      "7908      0.542324       0.813440               1.000000                    0   \n",
      "5617      0.476924       0.138941               0.000000                    0   \n",
      "5538      0.965045       0.552658               1.000000                    0   \n",
      "8133      0.967038       0.228742               0.500000                    0   \n",
      "6783      1.000000       0.190694               0.200000                    0   \n",
      "2697      0.000000       0.248575               0.000000                    0   \n",
      "1487      0.554022       0.451305               0.238095                    0   \n",
      "7058      0.701542       0.466978               0.133333                    0   \n",
      "6874      0.324689       0.299914               0.000000                    0   \n",
      "6797      0.124256       0.077896               0.500000                    0   \n",
      "1373      0.137244       0.221059               0.000000                    0   \n",
      "1505      0.216100       0.000000               0.200000                    0   \n",
      "3997      0.373655       0.000000               0.300000                    0   \n",
      "6902      0.164676       0.000000               0.025000                    0   \n",
      "8199      0.000000       0.235643               0.075000                    0   \n",
      "1360      0.314499       0.875250               0.000000                    0   \n",
      "2548      0.407863       0.496300               0.000000                    0   \n",
      "5875      0.025257       0.295898               0.000000                    0   \n",
      "6546      0.154386       0.000000               0.000000                    0   \n",
      "7941      0.065041       0.217796               0.025000                    0   \n",
      "9572      0.354305       0.000000               0.400000                    0   \n",
      "9397      0.635106       0.081699               0.400000                    0   \n",
      "2259      0.468262       0.665777               0.500000                    0   \n",
      "3365      0.461056       0.541155               0.000000                    0   \n",
      "5505      0.278466       0.512354               0.600000                    0   \n",
      "6404      1.000000       0.000000               0.200000                    0   \n",
      "8873      0.163246       0.149105               0.000000                    0   \n",
      "2089      0.143676       0.599733               0.000000                    0   \n",
      "7480      0.407111       0.198570               0.000000                    0   \n",
      "\n",
      "      IsFeaturedMerchant  IsWinner  \n",
      "7315                   1         0  \n",
      "7171                   0         0  \n",
      "9257                   0         0  \n",
      "2821                   1         0  \n",
      "2102                   0         0  \n",
      "837                    1         0  \n",
      "6241                   1         0  \n",
      "1057                   1         0  \n",
      "785                    1         0  \n",
      "7739                   1         0  \n",
      "7108                   1         0  \n",
      "5618                   1         0  \n",
      "6425                   1         0  \n",
      "4840                   1         0  \n",
      "6968                   1         1  \n",
      "6120                   1         0  \n",
      "209                    0         0  \n",
      "2389                   1         0  \n",
      "4597                   0         0  \n",
      "6803                   1         0  \n",
      "1427                   1         0  \n",
      "9316                   1         0  \n",
      "5243                   1         0  \n",
      "3098                   1         0  \n",
      "5810                   1         0  \n",
      "7972                   1         0  \n",
      "6065                   1         0  \n",
      "5097                   1         0  \n",
      "5402                   1         0  \n",
      "747                    1         0  \n",
      "...                  ...       ...  \n",
      "6952                   0         0  \n",
      "7908                   0         0  \n",
      "5617                   1         0  \n",
      "5538                   0         0  \n",
      "8133                   1         0  \n",
      "6783                   1         0  \n",
      "2697                   0         0  \n",
      "1487                   1         0  \n",
      "7058                   1         0  \n",
      "6874                   1         0  \n",
      "6797                   1         0  \n",
      "1373                   1         0  \n",
      "1505                   1         0  \n",
      "3997                   0         0  \n",
      "6902                   1         1  \n",
      "8199                   1         1  \n",
      "1360                   1         0  \n",
      "2548                   1         0  \n",
      "5875                   1         0  \n",
      "6546                   1         0  \n",
      "7941                   0         0  \n",
      "9572                   1         0  \n",
      "9397                   0         0  \n",
      "2259                   1         0  \n",
      "3365                   1         0  \n",
      "5505                   0         0  \n",
      "6404                   1         0  \n",
      "8873                   1         0  \n",
      "2089                   0         0  \n",
      "7480                   1         0  \n",
      "\n",
      "[6920 rows x 6 columns]\n",
      "\n",
      "Test data:\n",
      "       ListingPrice  ShippingPrice  ShippingTime_maxHours  IsFulfilledByAmazon  \\\n",
      "2797      0.147613       0.678166               0.200000                    0   \n",
      "7085      0.258362       0.000000               0.400000                    0   \n",
      "9091      0.258362       0.000000               0.400000                    0   \n",
      "2191      0.077753       0.000000               0.000000                    0   \n",
      "6712      0.387090       0.413906               0.000000                    0   \n",
      "3174      1.000000       0.473684               0.000000                    0   \n",
      "5099      0.786040       1.000000               0.075000                    0   \n",
      "8920      0.436818       0.244189               0.075000                    0   \n",
      "9669      0.061013       0.296529               0.400000                    0   \n",
      "4803      0.746639       0.757208               1.000000                    0   \n",
      "6584      0.286298       0.304289               0.000000                    0   \n",
      "4518      0.178699       0.557962               0.000000                    0   \n",
      "6713      0.553686       0.161364               0.050000                    0   \n",
      "5815      0.244270       0.000000               0.000000                    0   \n",
      "8025      0.022994       0.000000               0.333333                    0   \n",
      "9247      0.837500       0.000000               0.400000                    0   \n",
      "2408      0.052817       1.000000               0.400000                    0   \n",
      "5686      0.209740       0.263971               0.000000                    0   \n",
      "9596      0.503052       0.000000               0.000000                    0   \n",
      "7496      0.335566       0.235592               0.025000                    0   \n",
      "432       0.308751       0.000000               0.200000                    0   \n",
      "9637      0.796633       0.000000               0.000000                    0   \n",
      "7017      0.635722       0.075951               0.075000                    0   \n",
      "4117      0.048149       0.766968               0.200000                    0   \n",
      "3041      0.825686       0.000000               0.600000                    0   \n",
      "5139      0.190109       0.355034               0.375000                    0   \n",
      "6039      0.217283       0.000000               0.000000                    0   \n",
      "5890      0.793540       0.223717               0.375000                    0   \n",
      "7904      0.312714       0.234035               0.200000                    0   \n",
      "6868      0.000000       0.715081               0.000000                    0   \n",
      "...            ...            ...                    ...                  ...   \n",
      "1440      1.000000       0.000000               0.800000                    0   \n",
      "4856      0.077498       0.237492               0.250000                    0   \n",
      "8224      0.645421       0.167787               1.000000                    0   \n",
      "6771      0.197985       0.532037               0.200000                    0   \n",
      "7564      0.562470       0.333556               0.400000                    0   \n",
      "1255      0.319422       0.000000               1.000000                    0   \n",
      "6035      1.000000       0.000000               0.200000                    0   \n",
      "2013      0.775101       0.146456               0.000000                    0   \n",
      "1246      0.000000       0.000000               1.000000                    0   \n",
      "6684      0.476350       0.000000               0.000000                    0   \n",
      "6842      0.906555       0.000000               0.400000                    0   \n",
      "2699      0.070566       0.203704               1.000000                    0   \n",
      "997       0.353718       0.134264               0.200000                    0   \n",
      "6741      0.858595       0.000000               0.333333                    0   \n",
      "1223      0.436818       0.244189               0.075000                    0   \n",
      "2228      0.224364       0.000000               0.075000                    0   \n",
      "5370      0.392274       0.173638               0.375000                    0   \n",
      "3079      0.286298       0.304289               0.000000                    0   \n",
      "2273      0.126342       0.534865               0.000000                    0   \n",
      "5343      0.005558       0.000000               1.000000                    1   \n",
      "5555      0.926152       0.000000               0.000000                    0   \n",
      "8066      0.469709       0.000000               1.000000                    1   \n",
      "3050      0.247986       0.000000               1.000000                    0   \n",
      "9526      0.101704       0.000000               0.500000                    0   \n",
      "4187      0.268948       0.434480               0.075000                    0   \n",
      "3915      0.038598       0.408013               0.500000                    0   \n",
      "4271      0.157278       0.225775               0.200000                    0   \n",
      "2598      0.100429       0.000000               1.000000                    1   \n",
      "9711      0.590307       0.000000               0.200000                    0   \n",
      "686       0.136743       0.182615               0.000000                    0   \n",
      "\n",
      "      IsFeaturedMerchant  IsWinner  \n",
      "2797                   1         0  \n",
      "7085                   0         0  \n",
      "9091                   0         0  \n",
      "2191                   1         0  \n",
      "6712                   1         0  \n",
      "3174                   1         0  \n",
      "5099                   1         0  \n",
      "8920                   1         0  \n",
      "9669                   1         0  \n",
      "4803                   1         0  \n",
      "6584                   1         0  \n",
      "4518                   1         0  \n",
      "6713                   0         0  \n",
      "5815                   0         0  \n",
      "8025                   1         0  \n",
      "9247                   1         0  \n",
      "2408                   1         0  \n",
      "5686                   1         0  \n",
      "9596                   1         0  \n",
      "7496                   1         0  \n",
      "432                    1         0  \n",
      "9637                   1         0  \n",
      "7017                   1         0  \n",
      "4117                   1         0  \n",
      "3041                   1         0  \n",
      "5139                   0         0  \n",
      "6039                   0         0  \n",
      "5890                   1         0  \n",
      "7904                   1         0  \n",
      "6868                   0         0  \n",
      "...                  ...       ...  \n",
      "1440                   1         0  \n",
      "4856                   1         0  \n",
      "8224                   1         0  \n",
      "6771                   1         0  \n",
      "7564                   1         0  \n",
      "1255                   1         0  \n",
      "6035                   1         0  \n",
      "2013                   1         0  \n",
      "1246                   1         0  \n",
      "6684                   0         0  \n",
      "6842                   1         0  \n",
      "2699                   1         0  \n",
      "997                    1         0  \n",
      "6741                   1         0  \n",
      "1223                   1         0  \n",
      "2228                   1         0  \n",
      "5370                   1         0  \n",
      "3079                   1         0  \n",
      "2273                   1         0  \n",
      "5343                   1         0  \n",
      "5555                   1         0  \n",
      "8066                   1         0  \n",
      "3050                   1         1  \n",
      "9526                   1         0  \n",
      "4187                   1         0  \n",
      "3915                   1         0  \n",
      "4271                   1         0  \n",
      "2598                   1         0  \n",
      "9711                   1         0  \n",
      "686                    1         0  \n",
      "\n",
      "[2966 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print(\"Training data:\\n\", pd.concat([X_train, y_train], axis=1))\n",
    "print(\"\\nTest data:\\n\", pd.concat([X_test, y_test], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.25482756 -3.17654727 -2.21381541  2.27341652  2.73977289]]\n"
     ]
    }
   ],
   "source": [
    "#Train on the training sample and test on the test sample.\n",
    "logreg_train = LogisticRegression().fit(X_train, y_train)\n",
    "# Print the weights learned for each feature.\n",
    "print(logreg_train.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.98048435  0.01951565]\n",
      " [ 0.9968229   0.0031771 ]\n",
      " [ 0.9968229   0.0031771 ]\n",
      " ..., \n",
      " [ 0.71462009  0.28537991]\n",
      " [ 0.99313395  0.00686605]\n",
      " [ 0.86069372  0.13930628]]\n"
     ]
    }
   ],
   "source": [
    "# Estimated class probabilities on test set\n",
    "print(logreg_train.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Estimated classes  on test set\n",
    "predicted = logreg_train.predict(X_test)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.966958867161\n",
      "Confusion matrix: \n",
      " [[2809    4]\n",
      " [  94   59]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      2813\n",
      "          1       0.94      0.39      0.55       153\n",
      "\n",
      "avg / total       0.97      0.97      0.96      2966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score(y_test,predicted))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, predicted))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive features:\n",
      "       ListingPrice  ShippingPrice  ShippingTime_maxHours  IsFulfilledByAmazon\n",
      "0         0.000000       0.000000                  1.000                    1\n",
      "1         0.235077       0.000000                  0.025                    0\n",
      "2         0.113752       0.466311                  0.000                    0\n",
      "3         0.092270       0.778519                  0.000                    0\n",
      "4         0.272583       0.599733                  0.000                    0\n",
      "5         0.399718       0.465644                  0.000                    0\n",
      "6         0.540412       0.000000                  0.000                    0\n",
      "7         0.459236       0.665777                  0.075                    0\n",
      "8         0.558725       0.333556                  0.000                    0\n",
      "9         0.480190       0.665777                  0.075                    0\n",
      "10        0.688854       0.000000                  0.075                    0\n",
      "11        0.524212       0.863909                  0.000                    0\n",
      "12        0.802606       0.000000                  0.000                    0\n",
      "13        0.752069       0.369580                  0.000                    0\n",
      "14        0.655925       1.000000                  0.000                    0\n",
      "15        0.814580       0.466978                  0.000                    0\n",
      "16        0.914422       0.089393                  0.075                    0\n",
      "17        0.990843       0.000000                  0.050                    0\n",
      "18        1.000000       0.000000                  0.050                    0\n",
      "19        0.015816       0.217796                  0.000                    0\n",
      "20        0.053887       0.000000                  0.075                    0\n",
      "21        0.058109       0.000000                  1.000                    1\n",
      "22        0.025680       0.197780                  0.075                    0\n",
      "23        0.029383       0.197780                  0.075                    0\n",
      "24        0.000000       0.429845                  0.000                    0\n",
      "25        0.040424       0.224138                  0.000                    0\n",
      "26        0.069461       0.099088                  0.000                    0\n",
      "27        0.080916       0.099088                  0.000                    0\n",
      "28        0.099121       0.261395                  0.075                    0\n",
      "29        0.121340       0.138724                  0.000                    0\n",
      "...            ...            ...                    ...                  ...\n",
      "9856      0.061013       0.296529                  0.400                    0\n",
      "9857      0.149015       0.222844                  0.400                    0\n",
      "9858      0.254797       0.111982                  0.400                    0\n",
      "9859      0.074188       0.475924                  0.400                    0\n",
      "9860      0.408416       0.154311                  0.400                    0\n",
      "9861      0.468790       0.167973                  0.800                    0\n",
      "9862      0.520594       0.265174                  1.000                    0\n",
      "9863      0.698516       0.134378                  0.400                    0\n",
      "9864      0.635201       0.341769                  0.400                    0\n",
      "9865      1.000000       0.000000                  0.400                    0\n",
      "9866      0.697109       1.000000                  1.000                    0\n",
      "9867      0.000000       0.251288                  0.400                    0\n",
      "9868      0.050013       0.178947                  0.600                    0\n",
      "9869      0.037350       0.201344                  0.400                    0\n",
      "9870      0.160527       0.000000                  0.000                    1\n",
      "9871      0.161678       0.000000                  1.000                    0\n",
      "9872      0.039908       0.223516                  1.000                    0\n",
      "9873      0.178946       0.000000                  0.400                    0\n",
      "9874      0.182144       0.000000                  0.400                    0\n",
      "9875      0.058199       0.223516                  1.000                    0\n",
      "9876      0.061013       0.296529                  0.400                    0\n",
      "9877      0.149015       0.222844                  0.400                    0\n",
      "9878      0.254797       0.111982                  0.400                    0\n",
      "9879      0.074188       0.475924                  0.400                    0\n",
      "9880      0.408416       0.154311                  0.400                    0\n",
      "9881      0.468790       0.167973                  0.800                    0\n",
      "9882      0.520594       0.265174                  1.000                    0\n",
      "9883      0.698516       0.134378                  0.400                    0\n",
      "9884      0.635201       0.341769                  0.400                    0\n",
      "9885      1.000000       0.000000                  0.400                    0\n",
      "\n",
      "[9886 rows x 4 columns]\n",
      "\n",
      "Target feature:\n",
      " 0       0\n",
      "1       1\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      0\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      0\n",
      "18      0\n",
      "19      0\n",
      "20      1\n",
      "21      0\n",
      "22      0\n",
      "23      0\n",
      "24      0\n",
      "25      0\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "       ..\n",
      "9856    0\n",
      "9857    0\n",
      "9858    0\n",
      "9859    0\n",
      "9860    0\n",
      "9861    0\n",
      "9862    0\n",
      "9863    0\n",
      "9864    0\n",
      "9865    0\n",
      "9866    0\n",
      "9867    0\n",
      "9868    0\n",
      "9869    0\n",
      "9870    1\n",
      "9871    0\n",
      "9872    0\n",
      "9873    0\n",
      "9874    0\n",
      "9875    0\n",
      "9876    0\n",
      "9877    0\n",
      "9878    0\n",
      "9879    0\n",
      "9880    0\n",
      "9881    0\n",
      "9882    0\n",
      "9883    0\n",
      "9884    0\n",
      "9885    0\n",
      "Name: IsWinner, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare the descriptive features\n",
    "X = pd.concat([df[['ListingPrice','ShippingPrice','ShippingTime_maxHours','IsFulfilledByAmazon']]], axis=1)\n",
    "y = df.IsWinner \n",
    "print(\"Descriptive features:\\n\", X)\n",
    "print(\"\\nTarget feature:\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "       ListingPrice  ShippingPrice  ShippingTime_maxHours  IsFulfilledByAmazon  \\\n",
      "5297      0.353858       0.451789               0.000000                    0   \n",
      "8396      0.305181       1.000000               0.200000                    0   \n",
      "9561      0.123930       1.000000               0.400000                    0   \n",
      "859       0.333897       0.310238               0.200000                    0   \n",
      "3128      0.303684       0.111982               0.200000                    0   \n",
      "3948      0.065625       0.374759               1.000000                    0   \n",
      "1215      0.298875       0.231955               0.025000                    0   \n",
      "317       1.000000       0.000000               0.200000                    0   \n",
      "5050      0.376383       0.139494               0.000000                    0   \n",
      "6547      0.160668       0.000000               0.000000                    0   \n",
      "3699      0.223951       0.000000               0.200000                    0   \n",
      "6021      0.119153       0.179405               0.200000                    0   \n",
      "7421      0.196525       0.488860               0.000000                    0   \n",
      "7176      0.264268       1.000000               0.066667                    0   \n",
      "8483      0.360922       0.085106               0.000000                    0   \n",
      "1659      0.371155       0.000000               0.000000                    0   \n",
      "3047      1.000000       0.233100               0.400000                    0   \n",
      "9556      0.616570       0.000000               0.400000                    0   \n",
      "10        0.688854       0.000000               0.075000                    0   \n",
      "6356      0.708441       0.317632               0.119048                    0   \n",
      "4508      0.775374       0.166667               0.000000                    0   \n",
      "5079      1.000000       0.000000               0.333333                    0   \n",
      "4509      0.926477       0.166667               0.000000                    0   \n",
      "6826      0.644144       0.104798               0.000000                    0   \n",
      "5523      0.186258       0.000000               0.500000                    0   \n",
      "2702      0.374807       0.000000               0.000000                    0   \n",
      "8883      0.367073       0.570825               1.000000                    0   \n",
      "4588      1.000000       0.166667               0.000000                    0   \n",
      "2141      0.899249       0.134003               0.075000                    0   \n",
      "6548      0.159510       0.061861               0.075000                    0   \n",
      "...            ...            ...                    ...                  ...   \n",
      "469       0.608444       0.307152               0.400000                    0   \n",
      "8880      0.293782       0.124254               0.000000                    0   \n",
      "838       0.396328       0.000000               1.000000                    0   \n",
      "988       0.000000       0.138292               0.500000                    0   \n",
      "1722      0.843623       0.638582               0.000000                    0   \n",
      "7181      0.559611       0.091829               0.166667                    0   \n",
      "2914      0.325850       0.296870               1.000000                    0   \n",
      "310       0.411305       0.000000               0.200000                    0   \n",
      "31        0.086835       0.463337               0.200000                    0   \n",
      "2267      0.137726       0.058302               0.000000                    0   \n",
      "2295      0.480491       0.619005               0.500000                    0   \n",
      "6204      0.389591       1.000000               0.400000                    0   \n",
      "7581      0.492497       0.000000               0.025000                    0   \n",
      "9473      0.030732       0.216306               0.000000                    0   \n",
      "8612      0.094997       0.000000               0.000000                    1   \n",
      "8393      0.500924       0.000000               0.200000                    0   \n",
      "4861      0.176190       0.387904               0.000000                    0   \n",
      "3999      0.385854       0.164629               0.500000                    0   \n",
      "7399      0.039404       0.167612               1.000000                    0   \n",
      "7409      0.132401       0.293174               0.000000                    0   \n",
      "179       0.448134       0.000000               0.000000                    0   \n",
      "4405      0.591287       0.641089               0.000000                    0   \n",
      "9716      0.656629       0.896090               1.000000                    0   \n",
      "6452      0.336761       0.280101               0.075000                    0   \n",
      "5238      0.211062       0.000000               0.200000                    0   \n",
      "2750      0.769184       0.659199               0.400000                    0   \n",
      "4186      0.450976       0.000000               0.000000                    0   \n",
      "5364      0.202169       0.197188               0.375000                    0   \n",
      "2560      0.064134       0.000000               0.300000                    0   \n",
      "6760      1.000000       0.000000               0.200000                    0   \n",
      "\n",
      "      IsWinner  \n",
      "5297         0  \n",
      "8396         0  \n",
      "9561         0  \n",
      "859          0  \n",
      "3128         0  \n",
      "3948         0  \n",
      "1215         0  \n",
      "317          0  \n",
      "5050         0  \n",
      "6547         0  \n",
      "3699         0  \n",
      "6021         0  \n",
      "7421         0  \n",
      "7176         0  \n",
      "8483         0  \n",
      "1659         0  \n",
      "3047         0  \n",
      "9556         0  \n",
      "10           0  \n",
      "6356         0  \n",
      "4508         0  \n",
      "5079         0  \n",
      "4509         0  \n",
      "6826         0  \n",
      "5523         0  \n",
      "2702         0  \n",
      "8883         0  \n",
      "4588         0  \n",
      "2141         0  \n",
      "6548         0  \n",
      "...        ...  \n",
      "469          0  \n",
      "8880         0  \n",
      "838          0  \n",
      "988          1  \n",
      "1722         0  \n",
      "7181         0  \n",
      "2914         0  \n",
      "310          0  \n",
      "31           0  \n",
      "2267         0  \n",
      "2295         0  \n",
      "6204         0  \n",
      "7581         0  \n",
      "9473         0  \n",
      "8612         1  \n",
      "8393         0  \n",
      "4861         0  \n",
      "3999         0  \n",
      "7399         0  \n",
      "7409         0  \n",
      "179          0  \n",
      "4405         0  \n",
      "9716         0  \n",
      "6452         0  \n",
      "5238         0  \n",
      "2750         0  \n",
      "4186         0  \n",
      "5364         0  \n",
      "2560         0  \n",
      "6760         0  \n",
      "\n",
      "[6920 rows x 5 columns]\n",
      "\n",
      "Test data:\n",
      "       ListingPrice  ShippingPrice  ShippingTime_maxHours  IsFulfilledByAmazon  \\\n",
      "8481      0.213036       0.000000               0.075000                    0   \n",
      "9456      0.120079       0.000000               0.375000                    0   \n",
      "4997      1.000000       0.062416               0.075000                    0   \n",
      "7149      0.899249       0.134003               0.075000                    0   \n",
      "4367      0.151488       0.335722               0.200000                    0   \n",
      "3633      0.071309       0.000000               1.000000                    1   \n",
      "6738      0.582099       0.150451               0.666667                    0   \n",
      "8783      0.211696       0.333556               0.200000                    0   \n",
      "6052      0.866729       0.000000               0.000000                    0   \n",
      "6867      0.249168       0.000000               0.000000                    0   \n",
      "5665      0.143573       0.436408               0.200000                    0   \n",
      "6047      0.422000       0.268446               0.000000                    0   \n",
      "6002      0.241725       0.450768               1.000000                    0   \n",
      "5710      0.492508       0.000000               1.000000                    0   \n",
      "5363      0.000000       0.000000               0.000000                    0   \n",
      "604       0.000000       0.000000               1.000000                    1   \n",
      "8886      0.742478       0.407555               1.000000                    0   \n",
      "156       0.090093       0.453971               0.500000                    0   \n",
      "9197      0.071707       0.303214               0.000000                    0   \n",
      "6125      0.386729       0.258532               0.200000                    0   \n",
      "4434      0.322919       0.000000               1.000000                    0   \n",
      "8559      0.752394       0.183621               0.400000                    0   \n",
      "3017      0.392274       0.173638               0.375000                    0   \n",
      "7170      0.073724       0.184211               0.100000                    0   \n",
      "9336      0.114660       0.173104               0.200000                    0   \n",
      "9220      0.223941       0.223531               0.000000                    0   \n",
      "5912      0.417530       0.850795               0.200000                    0   \n",
      "4163      0.000000       0.223016               0.000000                    0   \n",
      "5786      0.495297       0.138173               0.000000                    0   \n",
      "9608      0.088690       0.239789               0.375000                    0   \n",
      "...            ...            ...                    ...                  ...   \n",
      "8651      0.000067       0.000000               0.000000                    1   \n",
      "9748      0.575282       0.171748               1.000000                    0   \n",
      "8188      0.000000       1.000000               0.000000                    0   \n",
      "2736      1.000000       0.000000               0.000000                    0   \n",
      "2933      0.433621       0.000000               0.000000                    1   \n",
      "774       0.549798       0.000000               0.300000                    0   \n",
      "7321      0.121593       0.000000               1.000000                    0   \n",
      "7958      0.201531       0.259875               0.050000                    0   \n",
      "2140      0.888571       0.046529               0.000000                    0   \n",
      "6171      0.044580       0.395525               0.133333                    0   \n",
      "959       0.521676       0.289129               0.000000                    0   \n",
      "394       0.189721       0.286496               0.025000                    0   \n",
      "6791      0.020294       0.255151               0.200000                    0   \n",
      "7151      1.000000       0.120882               0.000000                    0   \n",
      "9584      1.000000       0.123332               1.000000                    0   \n",
      "1142      0.198369       0.151930               0.000000                    0   \n",
      "9242      0.411875       0.321958               0.400000                    0   \n",
      "2917      0.397959       0.303251               0.375000                    0   \n",
      "8683      0.000000       0.455378               0.200000                    0   \n",
      "7522      0.577008       0.315667               0.125000                    0   \n",
      "2782      0.439104       0.000000               0.300000                    0   \n",
      "5027      0.174431       0.080815               0.200000                    0   \n",
      "5410      0.186366       0.000000               0.200000                    0   \n",
      "9692      0.337782       0.000000               0.000000                    0   \n",
      "3818      0.983661       0.204666               0.400000                    0   \n",
      "9204      0.408106       0.000000               0.000000                    0   \n",
      "3784      0.300757       0.000000               0.400000                    0   \n",
      "7711      0.123614       0.271348               0.000000                    0   \n",
      "1750      0.295570       0.000000               1.000000                    0   \n",
      "4942      0.117573       0.234475               0.200000                    0   \n",
      "\n",
      "      IsWinner  \n",
      "8481         0  \n",
      "9456         1  \n",
      "4997         0  \n",
      "7149         0  \n",
      "4367         0  \n",
      "3633         0  \n",
      "6738         0  \n",
      "8783         0  \n",
      "6052         0  \n",
      "6867         0  \n",
      "5665         0  \n",
      "6047         0  \n",
      "6002         0  \n",
      "5710         0  \n",
      "5363         1  \n",
      "604          0  \n",
      "8886         0  \n",
      "156          0  \n",
      "9197         0  \n",
      "6125         0  \n",
      "4434         0  \n",
      "8559         0  \n",
      "3017         0  \n",
      "7170         0  \n",
      "9336         0  \n",
      "9220         0  \n",
      "5912         0  \n",
      "4163         1  \n",
      "5786         0  \n",
      "9608         0  \n",
      "...        ...  \n",
      "8651         1  \n",
      "9748         0  \n",
      "8188         0  \n",
      "2736         0  \n",
      "2933         1  \n",
      "774          0  \n",
      "7321         0  \n",
      "7958         0  \n",
      "2140         0  \n",
      "6171         0  \n",
      "959          0  \n",
      "394          0  \n",
      "6791         0  \n",
      "7151         0  \n",
      "9584         0  \n",
      "1142         0  \n",
      "9242         0  \n",
      "2917         0  \n",
      "8683         0  \n",
      "7522         0  \n",
      "2782         0  \n",
      "5027         0  \n",
      "5410         0  \n",
      "9692         0  \n",
      "3818         0  \n",
      "9204         0  \n",
      "3784         0  \n",
      "7711         0  \n",
      "1750         0  \n",
      "4942         0  \n",
      "\n",
      "[2966 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print(\"Training data:\\n\", pd.concat([X_train, y_train], axis=1))\n",
    "print(\"\\nTest data:\\n\", pd.concat([X_test, y_test], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Refit the model on the training set only\n",
    "dtc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ActualClass</th>\n",
       "      <th>PredictedClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8481</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9456</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3633</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8783</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6867</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5665</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6047</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6002</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5710</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5363</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8886</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9197</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6125</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4434</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8559</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3017</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7170</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9336</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9220</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5912</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4163</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5786</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9608</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8651</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9748</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8188</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7321</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7958</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2140</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6171</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6791</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7151</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9584</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9242</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8683</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7522</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2782</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5027</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5410</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9692</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3818</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9204</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3784</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7711</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4942</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2966 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ActualClass  PredictedClass\n",
       "8481            0               0\n",
       "9456            1               0\n",
       "4997            0               0\n",
       "7149            0               0\n",
       "4367            0               0\n",
       "3633            0               0\n",
       "6738            0               0\n",
       "8783            0               0\n",
       "6052            0               0\n",
       "6867            0               0\n",
       "5665            0               0\n",
       "6047            0               0\n",
       "6002            0               0\n",
       "5710            0               0\n",
       "5363            1               1\n",
       "604             0               0\n",
       "8886            0               0\n",
       "156             0               0\n",
       "9197            0               0\n",
       "6125            0               0\n",
       "4434            0               0\n",
       "8559            0               0\n",
       "3017            0               0\n",
       "7170            0               0\n",
       "9336            0               0\n",
       "9220            0               0\n",
       "5912            0               0\n",
       "4163            1               0\n",
       "5786            0               0\n",
       "9608            0               0\n",
       "...           ...             ...\n",
       "8651            1               1\n",
       "9748            0               0\n",
       "8188            0               0\n",
       "2736            0               0\n",
       "2933            1               1\n",
       "774             0               0\n",
       "7321            0               0\n",
       "7958            0               0\n",
       "2140            0               0\n",
       "6171            0               0\n",
       "959             0               0\n",
       "394             0               0\n",
       "6791            0               0\n",
       "7151            0               0\n",
       "9584            0               0\n",
       "1142            0               0\n",
       "9242            0               0\n",
       "2917            0               0\n",
       "8683            0               0\n",
       "7522            0               0\n",
       "2782            0               0\n",
       "5027            0               0\n",
       "5410            0               0\n",
       "9692            0               0\n",
       "3818            0               0\n",
       "9204            0               0\n",
       "3784            0               0\n",
       "7711            0               0\n",
       "1750            0               0\n",
       "4942            0               0\n",
       "\n",
       "[2966 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on the hold-out test set\n",
    "predictions_test = dtc.predict(X_test)\n",
    "df_true_vs_predicted_test = pd.DataFrame({'ActualClass': y_test, 'PredictedClass': predictions_test})\n",
    "\n",
    "df_true_vs_predicted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.963250168577\n",
      "Confusion matrix: \n",
      " [[2782   16]\n",
      " [  93   75]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98      2798\n",
      "          1       0.82      0.45      0.58       168\n",
      "\n",
      "avg / total       0.96      0.96      0.96      2966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, predictions_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, predictions_test))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, predictions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Discuss how does evaluation on the test set compare to evaluation using the full data for training and also for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to evaluation on the test set and the full data, we could see that using full data get better model. \n",
    "While we could see that as we have big data set, using the full data or test set, the results are not big different. \n",
    "For Random Forest and Logistic Regression model, the total precision is the same 96% and for Linear Regression, its precision changed from 95% to 94%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Summarize and try to improve your results so far:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 Which model performs best and is it more accurate than a simple (but useless) model that always predicts IsWinner=0? Justify your answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer:  Random Forest is the best model. It is more accurate than Linear Regression model which is simple (but useless) model that always predicts IsWinner=0. Logistic Regression model also got very good result. Compared to Logistic Regression model, in the Confusion matrix, we could see that Random Forest actually got a little less errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Discuss your understanding of the problem and predictive modeling results so far. Can you find any tricks to improve the best model so far (e.g., using feature significance, feature re-scaling, creating new features, combining models, or other knowledge)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: After all the modeling, we found that our data is very good for the modeling. \n",
    "For the Amazon product ranking, we know that ListingPrice, IsFulfilledByAmazon, ShippingTime_maxHours, ShippingPric, ShippingTime_minHours those features have  significance impact to choose the winner of a product.\n",
    "\n",
    "For the problem, as the requirement is to choose winner, it requires to classify the data, so we could see that Logistic Regression model and Random Forest work better than Logistic Regression model(Which is better for continous feature). \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
